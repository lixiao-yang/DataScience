{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4aed5500-894c-47ef-9a77-e8e94d9c1892",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<h1 style=\"text-align:center\"> INFO 323: Cloud Computing and Big Data</h1>\n",
    "<h2 style=\"text-align:center\"> College of Computing and Informatics</h2>\n",
    "<h2 style=\"text-align:center\">Drexel University</h2>\n",
    "\n",
    "<h3 style=\"text-align:center\"> Aggregations</h3>\n",
    "<h3 style=\"text-align:center\"> Yuan An, PhD</h3>\n",
    "<h3 style=\"text-align:center\">Associate Professor</h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a35da274-1694-474b-a3c9-0c196ae90b85",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[1]: &#39;3.1.2&#39;</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[1]: &#39;3.1.2&#39;</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "904ae86d-812e-4782-80a4-fc2ef319f41f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let’s begin by reading in our data on purchases, repartitioning the data to have far fewer partitions\n",
    "(because we know it’s a small volume of data stored in a lot of small files), and caching the results\n",
    "for rapid access:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "65d0e538-925d-4a09-a3a7-022476b0b3ee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "filepath = \"dbfs:/FileStore/tables/retail-data/by-day/*.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a3f5f8fa-05ba-4744-92e0-5c9bd28558c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.format(\"csv\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5f2c188f-d804-4419-b496-564490da901d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[4]: 10</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[4]: 10</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# get the number of partitions of the DataFrame\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dbc7782c-a665-47b1-88e7-e64dafa177fd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Out[5]: 5</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Out[5]: 5</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Coalesce to 5 partitions\n",
    "df = df.coalesce(5)\n",
    "df.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a4726eca-6887-4a27-b706-47fa1e7bd8c6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.cache()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a36de976-1a90-4445-a4db-97c21e489085",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## count\n",
    "The first function worth going over is count, except in this example it will perform as a\n",
    "transformation instead of an action. In this case, we can do one of two things: specify a specific\n",
    "column to count, or all the columns by using count(*) or count(1) to represent that we want to\n",
    "count every row as the literal one, as shown in this example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fb2f8406-1764-4c25-b765-7466aac6a76c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">+----------------+\n",
       "count(StockCode)|\n",
       "+----------------+\n",
       "          541909|\n",
       "+----------------+\n",
       "\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">+----------------+\n|count(StockCode)|\n+----------------+\n|          541909|\n+----------------+\n\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "df.select(count(\"StockCode\")).show() # 541909"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "00bbf9c7-784c-423d-88d4-5be83efac71e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## countDistinct\n",
    "Sometimes, the total number is not relevant; rather, it’s the number of unique groups that you want. To\n",
    "get this number, you can use the countDistinct function. This is a bit more relevant for individual\n",
    "columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b59a419-b225-4b91-92f3-57921a9b3755",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n|count(DISTINCT StockCode)|\n+-------------------------+\n|                     4070|\n+-------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import countDistinct\n",
    "df.select(countDistinct(\"StockCode\")).show() # 4070"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f8933805-5e9a-408b-922a-3a0513cd00e1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## approx_count_distinct\n",
    "Often, we find ourselves working with large datasets and the exact distinct count is irrelevant. There\n",
    "are times when an approximation to a certain degree of accuracy will work just fine, and for that, you\n",
    "can use the approx_count_distinct function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5a7818e8-f5d4-4ba2-9dda-9e11f9e405bb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n|approx_count_distinct(StockCode)|\n+--------------------------------+\n|                            3364|\n+--------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import approx_count_distinct\n",
    "df.select(approx_count_distinct(\"StockCode\", 0.1)).show() # 3364"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3ce5d8f6-a691-4086-8e75-fde8c785d278",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## first and last\n",
    "You can get the first and last values from a DataFrame by using these two obviously named functions.\n",
    "This will be based on the rows in the DataFrame, not on the values in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "05378a4c-617a-423d-8b70-93ce1c9b5678",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+---------------+\n|first(StockCode)|last(StockCode)|\n+----------------+---------------+\n|           23084|          22168|\n+----------------+---------------+\n\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import first, last\n",
    "df.select(first(\"StockCode\"), last(\"StockCode\")).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91600475-93c2-47a2-93f9-811a2d1028cd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[16]: Row(InvoiceNo='580538', StockCode='23084', Description='RABBIT NIGHT LIGHT', Quantity=48, InvoiceDate=datetime.datetime(2011, 12, 5, 8, 38), UnitPrice=1.79, CustomerID=14075.0, Country='United Kingdom')"
     ]
    }
   ],
   "source": [
    "# get the first row\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0374ecf2-c955-4ebd-a4a8-64f6511f64e5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get the Quantity of the first row\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "db29c951-2180-41d8-93cf-b7daabae3131",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## min and max\n",
    "To extract the minimum and maximum values from a DataFrame, use the min and max functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6840833f-9cd4-44f3-badf-a723ce2d9e0e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+\n|min(Quantity)|max(Quantity)|\n+-------------+-------------+\n|       -80995|        80995|\n+-------------+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import min, max\n",
    "df.select(min(\"Quantity\"), max(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8dc3382d-30b9-4477-912f-cc1c6edde111",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## sum\n",
    "Another simple task is to add all the values in a row using the sum function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "959bc944-55d2-45ad-91fc-f500b64cfef4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n|sum(Quantity)|\n+-------------+\n|      5176450|\n+-------------+\n\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "df.select(sum(\"Quantity\")).show() # 5176450"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d655f163-c79a-4bea-8c2e-dacf75953b31",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## sumDistinct\n",
    "In addition to summing a total, you also can sum a distinct set of values by using the sumDistinct\n",
    "function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e08d5318-0a1f-4db8-87b9-58e7c70a5db1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------+\n|sum(DISTINCT Quantity)|\n+----------------------+\n|                 29310|\n+----------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import sumDistinct\n",
    "df.select(sumDistinct(\"Quantity\")).show() # 29310"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "dde5fc1f-0e59-40ab-889a-1f9b75563780",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## avg\n",
    "Although you can calculate average by dividing sum by count, Spark provides an easier way to get\n",
    "that value via the avg or mean functions. In this example, we use alias in order to more easily reuse\n",
    "these columns later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "822c8e56-4405-42f7-a953-ffd588ee0359",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------------+----------------+----------------+\n|(total_purchases / total_transactions)|   avg_purchases|  mean_purchases|\n+--------------------------------------+----------------+----------------+\n|                      9.55224954743324|9.55224954743324|9.55224954743324|\n+--------------------------------------+----------------+----------------+\n\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import sum, count, avg, expr\n",
    "\n",
    "df.select(\n",
    "    count(\"Quantity\").alias(\"total_transactions\"),\n",
    "    sum(\"Quantity\").alias(\"total_purchases\"),\n",
    "    avg(\"Quantity\").alias(\"avg_purchases\"),\n",
    "    expr(\"mean(Quantity)\").alias(\"mean_purchases\"))\\\n",
    "  .selectExpr(\n",
    "    \"total_purchases/total_transactions\",\n",
    "    \"avg_purchases\",\n",
    "    \"mean_purchases\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "77cf86aa-6243-4e0b-b2f6-9aa476e09c87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Variance and Standard Deviation\n",
    "Calculating the mean naturally brings up questions about the variance and standard deviation.  You\n",
    "can calculate these in Spark by using their respective functions. However, something to note is that\n",
    "Spark has both the formula for the sample standard deviation as well as the formula for the population\n",
    "standard deviation. These are fundamentally different statistical formulae, and we need to\n",
    "differentiate between them. By default, Spark performs the formula for the sample standard deviation\n",
    "or variance if you use the variance or stddev functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8eaa6445-b9c0-460f-a84c-262ad95c492a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+--------------------+---------------------+\n|var_pop(Quantity)|var_samp(Quantity)|stddev_pop(Quantity)|stddev_samp(Quantity)|\n+-----------------+------------------+--------------------+---------------------+\n|47559.30364660885| 47559.39140929855|   218.0809566344775|    218.0811578502337|\n+-----------------+------------------+--------------------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import var_pop, stddev_pop\n",
    "from pyspark.sql.functions import var_samp, stddev_samp\n",
    "df.select(var_pop(\"Quantity\"), var_samp(\"Quantity\"),\n",
    "  stddev_pop(\"Quantity\"), stddev_samp(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "2e57bd08-412a-4301-b2a0-cf505909d5b0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## skewness and kurtosis\n",
    "Skewness and kurtosis are both measurements of extreme points in your data. Skewness measures the\n",
    "asymmetry of the values in your data around the mean, whereas kurtosis is a measure of the tail of\n",
    "data. These are both relevant specifically when modeling your data as a probability distribution of a\n",
    "random variable. Although here we won’t go into the math behind these specifically, you can look up\n",
    "definitions quite easily on the internet. You can calculate these by using the functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bfc7a067-d0fc-429e-8815-4c60059e697b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n|skewness(Quantity)|kurtosis(Quantity)|\n+------------------+------------------+\n|-0.264075576105286|119768.05495534562|\n+------------------+------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import skewness, kurtosis\n",
    "df.select(skewness(\"Quantity\"), kurtosis(\"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8b9a1292-eedd-4065-bbea-459f3b245301",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Covariance and Correlation\n",
    "We discussed single column aggregations, but some functions compare the interactions of the values\n",
    "in two difference columns together. Two of these functions are cov and corr, for covariance and\n",
    "correlation, respectively. Correlation measures the Pearson correlation coefficient, which is scaled\n",
    "between –1 and +1. The covariance is scaled according to the inputs in the data.\n",
    "\n",
    "Like the var function, covariance can be calculated either as the sample covariance or the population\n",
    "covariance. Therefore it can be important to specify which formula you want to use. Correlation has\n",
    "no notion of this and therefore does not have calculations for population or sample. Here’s how they\n",
    "work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0dc1bd96-2fe0-4dd6-8b97-25e2e4c75aa1",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+-------------------------------+------------------------------+\n|corr(InvoiceNo, Quantity)|covar_samp(InvoiceNo, Quantity)|covar_pop(InvoiceNo, Quantity)|\n+-------------------------+-------------------------------+------------------------------+\n|     4.912186085617365E-4|             1052.7280543863135|            1052.7260778702093|\n+-------------------------+-------------------------------+------------------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import corr, covar_pop, covar_samp\n",
    "df.select(corr(\"InvoiceNo\", \"Quantity\"), covar_samp(\"InvoiceNo\", \"Quantity\"),\n",
    "    covar_pop(\"InvoiceNo\", \"Quantity\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "91285b1f-faf8-4a27-bc60-d8543a2b29bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Aggregating to Complex Types\n",
    "In Spark, you can perform aggregations not just of numerical values using formulas, you can also\n",
    "perform them on complex types. For example, we can collect a list of values present in a given\n",
    "column or only the unique values by collecting to a set.\n",
    "You can use this to carry out some more programmatic access later on in the pipeline or pass the\n",
    "entire collection in a user-defined function (UDF):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a08bcba0-a5d3-4aaa-9b14-41b46867337a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------------------+\n|collect_set(Country)|collect_list(Country)|\n+--------------------+---------------------+\n|[Portugal, Italy,...| [United Kingdom, ...|\n+--------------------+---------------------+\n\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import collect_set, collect_list\n",
    "df.agg(collect_set(\"Country\"), collect_list(\"Country\")).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "57f3dac4-5957-41ad-8ca7-a8d2a4f922c6",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Grouping\n",
    "Thus far, we have performed only DataFrame-level aggregations. A more common task is to perform\n",
    "calculations based on groups in the data. This is typically done on categorical data for which we\n",
    "group our data on one column and perform some calculations on the other columns that end up in that\n",
    "group.\n",
    "\n",
    "The best way to explain this is to begin performing some groupings. The first will be a count, just as\n",
    "we did before. We will group by each unique invoice number and get the count of items on that\n",
    "invoice. Note that this returns another DataFrame and is lazily performed.\n",
    "\n",
    "We do this grouping in two phases. First we specify the column(s) on which we would like to group,\n",
    "and then we specify the aggregation(s). The first step returns a RelationalGroupedDataset, and the\n",
    "second step returns a DataFrame.\n",
    "\n",
    "As mentioned, we can specify any number of columns on which we want to group:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "fab73898-0d1b-473c-90c7-5f99e9facd9a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+-----+\n|InvoiceNo|CustomerId|count|\n+---------+----------+-----+\n|   580657|   14696.0|   20|\n|   581474|   12748.0|   24|\n|   576641|   17549.0|    4|\n|   575753|   17841.0|   80|\n|   576357|   15660.0|   18|\n|  C579018|   16678.0|    1|\n|   579062|   17651.0|   16|\n|   570413|   17652.0|   18|\n|  C569961|   13704.0|    1|\n|   572518|   16353.0|    1|\n|   577696|   16406.0|   63|\n|   577783|   18139.0|   35|\n|   580090|   13323.0|    5|\n|   577338|   17650.0|    8|\n|  C577375|   14441.0|    2|\n|   568670|   14978.0|   38|\n|   580138|   13199.0|    3|\n|  C538084|   12649.0|    2|\n|  C538085|   16350.0|    3|\n|   575200|   12921.0|    1|\n+---------+----------+-----+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import count\n",
    "df.groupBy(\"InvoiceNo\", \"CustomerId\").count().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c4115c87-abf3-406e-9c39-5aa99df79e3b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Grouping with Expressions\n",
    "As we saw earlier, counting is a bit of a special case because it exists as a method. For this, usually\n",
    "we prefer to use the count function. Rather than passing that function as an expression into a select\n",
    "statement, we specify it as within agg. This makes it possible for you to pass-in arbitrary expressions\n",
    "that just need to have some aggregation specified. You can even do things like alias a column after\n",
    "transforming it for later use in your data flow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "408d0c87-9978-41d4-9457-c38e87de0085",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------------+\n|InvoiceNo|quan|count(Quantity)|\n+---------+----+---------------+\n|   574966|   8|              8|\n|   575091|  38|             38|\n|   578057|  28|             28|\n|   537252|   1|              1|\n|   578459|   8|              8|\n|  C578132|   1|              1|\n|   578292|  72|             72|\n|   576112|  20|             20|\n|   577022|  38|             38|\n|   574592|   8|              8|\n|  C576393|   2|              2|\n|   577511|  46|             46|\n|   577541|  21|             21|\n|   580739|   2|              2|\n|   580906|   4|              4|\n|   573726|   1|              1|\n|   575671|  20|             20|\n|   570264|   1|              1|\n|   570281|   3|              3|\n|   569823|  69|             69|\n+---------+----+---------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "df.groupBy(\"InvoiceNo\").agg(\n",
    "    count(\"Quantity\").alias(\"quan\"),\n",
    "    expr(\"count(Quantity)\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7f1fff66-8cb5-426e-a71c-d066b95913f2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Grouping Sets\n",
    "We may want an aggregation across multiple groups. We achieve this by using grouping sets. Grouping\n",
    "sets are a low-level tool for combining sets of aggregations together. They give you the ability to\n",
    "create arbitrary aggregation in their group-by statements.\n",
    "\n",
    "Let’s work through an example to gain a better understanding. Here, we would like to get the total\n",
    "quantity of all stock codes and customers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a425db0a-a5d3-42fa-bd20-500d7f345a41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "dfNoNull = dfWithDate.drop()\n",
    "dfNoNull.createOrReplaceTempView(\"dfNoNull\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "20af0340-6072-4cae-8b01-da6aa7a5b9f5",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "```\n",
    "-- in SQL\n",
    "SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull\n",
    "GROUP BY customerId, stockCode\n",
    "ORDER BY CustomerId DESC, stockCode DESC\n",
    "```\n",
    "\n",
    "You can do the exact same thing by using a grouping set:\n",
    "\n",
    "```\n",
    "-- in SQL\n",
    "SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull\n",
    "GROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode))\n",
    "ORDER BY CustomerId DESC, stockCode DESC\n",
    "```\n",
    "\n",
    "Simple enough, but what if you also want to include the total number of items, regardless of customer\n",
    "or stock code? With a conventional group-by statement, this would be impossible. But, it’s simple\n",
    "with grouping sets: we simply specify that we would like to aggregate at that level, as well, in our\n",
    "grouping set. This is, effectively, the union of several different groupings together:\n",
    "\n",
    "```\n",
    "-- in SQL\n",
    "SELECT CustomerId, stockCode, sum(Quantity) FROM dfNoNull\n",
    "GROUP BY customerId, stockCode GROUPING SETS((customerId, stockCode),())\n",
    "ORDER BY CustomerId DESC, stockCode DESC\n",
    "```\n",
    "\n",
    "The GROUPING SETS operator is only available in SQL. To perform the same in DataFrames, you use\n",
    "the rollup and cube operators—which allow us to get the same results. Let’s go through those."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0524e087-77f2-4992-9def-2e863bdc6471",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Rollups\n",
    "A rollup is a multidimensional aggregation that performs a variety of group-by style calculations for us.\n",
    "Let’s create a rollup that looks across time (with our new Date column) and space (with the Country\n",
    "column) and creates a new DataFrame that includes the grand total over all dates, the grand total for\n",
    "each date in the DataFrame, and the subtotal for each country on each date in the DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "24d5eaa4-8bf8-414c-a4c5-745d1d7846be",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------+--------------+\n|      Date|       Country|total_quantity|\n+----------+--------------+--------------+\n|      null|          null|       5176450|\n|2010-12-01|   Netherlands|            97|\n|2010-12-01|          EIRE|           243|\n|2010-12-01|       Germany|           117|\n|2010-12-01|        France|           449|\n|2010-12-01|     Australia|           107|\n|2010-12-01|          null|         26814|\n|2010-12-01|United Kingdom|         23949|\n|2010-12-01|        Norway|          1852|\n|2010-12-02|          EIRE|             4|\n|2010-12-02|          null|         21023|\n|2010-12-02|       Germany|           146|\n|2010-12-02|United Kingdom|         20873|\n|2010-12-03|        Poland|           140|\n|2010-12-03|   Switzerland|           110|\n|2010-12-03|       Germany|           170|\n|2010-12-03|         Spain|           400|\n|2010-12-03|         Italy|           164|\n|2010-12-03|      Portugal|            65|\n|2010-12-03|          null|         14830|\n+----------+--------------+--------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "rolledUpDF = dfNoNull.rollup(\"Date\", \"Country\").agg(sum(\"Quantity\"))\\\n",
    "  .selectExpr(\"Date\", \"Country\", \"`sum(Quantity)` as total_quantity\")\\\n",
    "  .orderBy(\"Date\")\n",
    "rolledUpDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "70a3e37c-3974-47b4-a089-2375c50febb8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now where you see the null values is where you’ll find the grand totals. A null in both rollup\n",
    "columns specifies the grand total across both of those columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a99c3fa0-30b4-4e6a-915a-00708d28e504",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------+--------------+\n|      Date|Country|total_quantity|\n+----------+-------+--------------+\n|      null|   null|       5176450|\n|2010-12-01|   null|         26814|\n|2010-12-02|   null|         21023|\n|2010-12-03|   null|         14830|\n|2010-12-05|   null|         16395|\n|2010-12-06|   null|         21419|\n|2010-12-07|   null|         24995|\n|2010-12-08|   null|         22741|\n|2010-12-09|   null|         18431|\n|2010-12-10|   null|         20297|\n|2010-12-12|   null|         10565|\n|2010-12-13|   null|         17623|\n|2010-12-14|   null|         20098|\n|2010-12-15|   null|         18229|\n|2010-12-16|   null|         29632|\n|2010-12-17|   null|         16069|\n|2010-12-19|   null|          3795|\n|2010-12-20|   null|         14965|\n|2010-12-21|   null|         15467|\n|2010-12-22|   null|          3192|\n+----------+-------+--------------+\nonly showing top 20 rows\n\n+----+-------+--------------+\n|Date|Country|total_quantity|\n+----+-------+--------------+\n|null|   null|       5176450|\n+----+-------+--------------+\n\n"
     ]
    }
   ],
   "source": [
    "rolledUpDF.where(\"Country IS NULL\").show()\n",
    "rolledUpDF.where(\"Date IS NULL\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e7bb4246-4cf8-41bb-bdfe-8343a232f0aa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Cube\n",
    "A cube takes the rollup to a level deeper. Rather than treating elements hierarchically, a cube does the\n",
    "same thing across all dimensions. This means that it won’t just go by date over the entire time period,\n",
    "but also the country.\n",
    "\n",
    "To pose this as a question again, can you make a table that includes the\n",
    "following?\n",
    "* The total across all dates and countries\n",
    "* The total for each date across all countries\n",
    "* The total for each country on each date\n",
    "* The total for each country across all dates\n",
    "\n",
    "The method call is quite similar, but instead of calling rollup, we call cube:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cae6b979-6c1a-4dce-a1f4-fe78e4260adf",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+-------------+\n|Date|             Country|sum(Quantity)|\n+----+--------------------+-------------+\n|null|               Japan|        25218|\n|null|           Australia|        83653|\n|null|              Cyprus|         6317|\n|null|         Unspecified|         3300|\n|null|            Portugal|        16180|\n|null|                 RSA|          352|\n|null|             Germany|       117448|\n|null|     Channel Islands|         9479|\n|null|           Singapore|         5234|\n|null|                null|      5176450|\n|null|                 USA|         1034|\n|null|             Finland|        10666|\n|null|           Hong Kong|         4769|\n|null|             Lebanon|          386|\n|null|               Spain|        26824|\n|null|  European Community|          497|\n|null|             Denmark|         8188|\n|null|              Norway|        19247|\n|null|United Arab Emirates|          982|\n|null|      Czech Republic|          592|\n+----+--------------------+-------------+\nonly showing top 20 rows\n\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import sum\n",
    "\n",
    "dfNoNull.cube(\"Date\", \"Country\").agg(sum(col(\"Quantity\")))\\\n",
    "  .select(\"Date\", \"Country\", \"sum(Quantity)\").orderBy(\"Date\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "11a082f6-64fa-4440-ae06-0f9bfdee9091",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "INFO323-Lecture-Spark-4-Databricks-Aggregations",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
