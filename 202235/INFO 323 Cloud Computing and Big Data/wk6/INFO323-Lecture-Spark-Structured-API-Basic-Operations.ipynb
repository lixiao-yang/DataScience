{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LvuJLOMd-AKt"
   },
   "source": [
    "<h1 style=\"text-align:center\"> INFO 323: Cloud Computing and Big Data</h1>\n",
    "<h2 style=\"text-align:center\"> College of Computing and Informatics</h2>\n",
    "<h2 style=\"text-align:center\">Drexel University</h2>\n",
    "\n",
    "<h3 style=\"text-align:center\"> Structured API (Ch 4: Spark Structured API and Basic Operations)</h3>\n",
    "<h3 style=\"text-align:center\"> Yuan An, PhD</h3>\n",
    "<h3 style=\"text-align:center\">Associate Professor</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_fsygeiu-AK1"
   },
   "source": [
    "## Chapter 4 of Spark Definitive Guide"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEkX0-z6-AK2"
   },
   "source": [
    "## Overview\n",
    "- Structured APIs are a tool for manipulating all sorts of data, from unstructured log files to semi-structured CSV files and highly structured Parquet files.\n",
    "- These APIs refer to three core types of distributed collection APIs:\n",
    " - Datasets\n",
    " - DataFrames\n",
    " - SQL tables and views\n",
    "- The majority of the Structured APIs apply to both batch and streaming computation. \n",
    "- It should be simple to migrate from batch to streaming (or vice versa) with little to no effort. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FWyvij-5-AK2"
   },
   "source": [
    "## DataFrames and Datasets\n",
    "- Spark has two notions of structured collections: DataFrames and Datasets. \n",
    "- Although they have (nuanced) differences, both are (distributed) table-like collections with well-defined rows and columns. \n",
    "- DataFrames and Datasets represent immutable, lazily evaluated plans that specify what operations to apply to data residing at a location to generate some output. \n",
    "- When we perform an action on a DataFrame, we instruct Spark to perform the actual transformations and return the result. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VmlFH7h--AK3"
   },
   "source": [
    "## DataFrames and Datasets\n",
    "- DataSets only for Java and Scala\n",
    "- DataFrames are DataSets of type of Row\n",
    "- The “Row” type is Spark’s internal representation of its optimized in-memory format for computation. \n",
    "- This format makes for highly specialized and efficient computation.\n",
    "- To Spark (in Python or R), there is no such thing as a Dataset: everything is a DataFrame and therefore we always operate on that optimized format.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rIPVBFCi-AK4"
   },
   "source": [
    "## Data Types\n",
    "- Columns represent a simple type like an integer or string.\n",
    "- A row is nothing more than a record of data. \n",
    "- Each record in a DataFrame must be of type Row.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "0_rfXNya-AK4",
    "outputId": "5f35911d-04eb-43a4-8fe2-b4c3ea727972"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'3.1.3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "8G5gBlO2-AK5",
    "outputId": "9832c249-9639-43bc-a1bb-82eebe260cf5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(id=0), Row(id=1)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(2).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "szrNUITD-AK6"
   },
   "source": [
    "- Spark has a large number of internal type representations: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "wxmdnswd-AK6",
    "outputId": "402cdf48-6cee-49ae-d69f-925c75b6fab9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ByteType"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "b = ByteType()\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D_Rw4QS6-AK7"
   },
   "source": [
    "## Structured API Execution\n",
    "\n",
    "- Understanding how code is actually executed across a cluster will help us understand (and potentially debug) the process of writing and executing code on clusters:\n",
    "- An overview of the steps:\n",
    " - 1. Write DataFrame/Dataset/SQL Code.\n",
    " - 2. If valid code, Spark converts this to a Logical Plan.\n",
    " - 3. Spark transforms this Logical Plan to a Physical Plan, checking for optimizations along the way.\n",
    " - 4. Spark then executes this Physical Plan (RDD manipulations) on the cluster.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FMXxQg_b-AK7"
   },
   "source": [
    "## API References\n",
    "- Spark is a growing project, and any book is a snapshot in time. Find latest API:\n",
    "http://spark.apache.org/docs/latest/api/python/\n",
    "- org.apache.spark.sql.functions contains a variety of functions for a range of different data types. \n",
    "- The majority of these functions are ones that you will find in SQL and analytics systems.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iWJcXHbV-CL3"
   },
   "source": [
    "### Create a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "NjKO32-S-CL4"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"json\").load(\"gs://info323-ya45-spring2023/notebooks/jupyter/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "WIGXIfJH-CL4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(DEST_COUNTRY_NAME,StringType,true),StructField(ORIGIN_COUNTRY_NAME,StringType,true),StructField(count,LongType,true)))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qy7J1PR7-CL4"
   },
   "source": [
    "Show the schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jumHCTct-CL5"
   },
   "source": [
    "The example that follows shows how to create and enforce a\n",
    "specific schema on a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "OuGsZt6r-CL5"
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "\n",
    "myManualSchema = StructType([\n",
    "  StructField(\"DEST_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"ORIGIN_COUNTRY_NAME\", StringType(), True),\n",
    "  StructField(\"count\", LongType(), False, metadata={\"hello\":\"world\"})\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "OuGsZt6r-CL5"
   },
   "outputs": [],
   "source": [
    "df = spark.read.format(\"json\").schema(myManualSchema)\\\n",
    "  .load(\"gs://info323-ya45-spring2023/notebooks/jupyter/2015-summary.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FScjI6KP-CL6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xyhI4ldY-CL6"
   },
   "source": [
    "### Columns\n",
    "There are a lot of different ways to construct and refer to columns but the two simplest ways are by\n",
    "using the col or column functions. To use either of these functions, you pass in a column name:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "X4TnhlN4-CL6"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'somecol'>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col(\"somecol\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'someColumn'>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column(\"someColumn\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fr5LgB9K-CL7"
   },
   "source": [
    "### Columns as expressions\n",
    "Columns provide a subset of expression functionality. If you use col() and want to perform\n",
    "transformations on that column, you must perform those on that column reference. When using an\n",
    "expression, the expr function can actually parse transformations and column references from a string\n",
    "and can subsequently be passed into further transformations. Let’s look at some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "gdkoF9Jw-CL7"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'((((someCol + 5) * 200) - 6) < otherCol)'>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "expr(\"(((someCol + 5) * 200) - 6) < otherCol\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-43uEJic-CL7"
   },
   "source": [
    "### Records and Rows\n",
    "In Spark, each row in a DataFrame is a single record. Spark represents this record as an object of\n",
    "type Row. Spark manipulates Row objects using column expressions in order to produce usable values.\n",
    "Row objects internally represent arrays of bytes. The byte array interface is never shown to users\n",
    "because we only use column expressions to manipulate them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "f0BF5BLh-CL8"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRow = Row(\"Hello\", None, 1, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "myRow[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PwBQtZcD-CL8"
   },
   "source": [
    "Accessing data in rows is equally as easy: you just specify the position that you would like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "77dqTpUp-CL9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "myRow[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "77dqTpUp-CL9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "myRow[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2kn05mR-CL9"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3joUPZJf-CL9"
   },
   "source": [
    "### Creating DataFrames\n",
    "We can create DataFrames from raw data sources. We will also register this as a temporary view so that we can query it\n",
    "with SQL and show off basic transformations in SQL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import StructField, StructType, StringType, LongType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "mySchema = StructType([\n",
    "    StructField(\"Destination\", StringType(), True),\n",
    "    StructField(\"Origin\", StringType(), True),\n",
    "    StructField(\"count\", LongType(), False)\n",
    "]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "qyMqkeeN-CL9"
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df = spark.read.format(\"csv\").schema(mySchema).load(\"gs://info323-ya45-spring2023/notebooks/jupyter/2015-summary.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Destination: string (nullable = true)\n",
      " |-- Origin: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|         Destination|             Origin|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n",
      "|       United States|            Romania|   15|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|            Ireland|  344|\n",
      "|               Egypt|      United States|   15|\n",
      "|       United States|              India|   62|\n",
      "|       United States|          Singapore|    1|\n",
      "|       United States|            Grenada|   62|\n",
      "|          Costa Rica|      United States|  588|\n",
      "|             Senegal|      United States|   40|\n",
      "|             Moldova|      United States|    1|\n",
      "|       United States|       Sint Maarten|  325|\n",
      "|       United States|   Marshall Islands|   39|\n",
      "|              Guyana|      United States|   64|\n",
      "|               Malta|      United States|    1|\n",
      "|            Anguilla|      United States|   41|\n",
      "|             Bolivia|      United States|   30|\n",
      "|       United States|           Paraguay|    6|\n",
      "|             Algeria|      United States|    4|\n",
      "|Turks and Caicos ...|      United States|  230|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "qyMqkeeN-CL9"
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "4JDfCyct-CL-"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|      Destination|             Origin|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "essDX4Ih-CL-"
   },
   "source": [
    "We can also create DataFrames on the fly by taking a set of rows and converting them to a DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "DQjfklnb-CL-"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+-----+\n",
      "| some| col|names|\n",
      "+-----+----+-----+\n",
      "|Hello|null|    1|\n",
      "+-----+----+-----+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.types import StructField, StructType, StringType, LongType\n",
    "myManualSchema = StructType([\n",
    "  StructField(\"some\", StringType(), True),\n",
    "  StructField(\"col\", StringType(), True),\n",
    "  StructField(\"names\", LongType(), False)\n",
    "])\n",
    "myRow = Row(\"Hello\", None, 1)\n",
    "myDf = spark.createDataFrame([myRow], myManualSchema)\n",
    "myDf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GH-OpVvs-CL_"
   },
   "source": [
    "Use the select method and pass in the column names as\n",
    "strings with which you would like to work:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXzAkmsL-CMA"
   },
   "source": [
    "You can select multiple columns by using the same style of query, just add more column name strings\n",
    "to your select method call:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "oSbSY1se-CMA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+\n",
      "|      Destination|             Origin|\n",
      "+-----------------+-------------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|\n",
      "|    United States|            Romania|\n",
      "+-----------------+-------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.select(\"Destination\", \"Origin\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fb2x3jcP-CMA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yQDNcJ8v-CMA"
   },
   "source": [
    "You can refer to columns in a number of different ways;\n",
    "all you need to keep in mind is that you can use them interchangeably:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "ryh73oZP-CMA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+-----------------+\n",
      "|      Destination|      Destination|      Destination|\n",
      "+-----------------+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "|    United States|    United States|    United States|\n",
      "+-----------------+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import expr, col, column\n",
    "df.select(\n",
    "    expr(\"Destination\"),\n",
    "    col(\"Destination\"),\n",
    "    column(\"Destination\"))\\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h67ETvmw-CMB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ILZTl_2k-CMB"
   },
   "source": [
    "As we’ve seen thus far, expr is the most flexible reference that we can use. It can refer to a plain\n",
    "column or a string manipulation of a column. To illustrate, let’s change the column name, and then\n",
    "change it back by using the AS keyword and then the alias method on the column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "INRUOjIp-CMB"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.select(expr(\"Destination as DEST_COUNTRY_NAME\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "--OUtz1S-CMB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SMB10Zy-CMB"
   },
   "source": [
    "This changes the column name to “destination.” You can further manipulate the result of your\n",
    "expression as another expression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "eYgJ3LbZ-CMC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|      destination|\n",
      "+-----------------+\n",
      "|DEST_COUNTRY_NAME|\n",
      "|    United States|\n",
      "+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.select(expr(\"Destination as DEST_COUNTRY_NAME\").alias(\"destination\"))\\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "S75OCXjL-CMC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|      Destination|      Destination|\n",
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "|    United States|    United States|\n",
      "+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Destination\", col(\"Destination\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UIc-Eath-CMC"
   },
   "source": [
    "Because select followed by a series of expr is such a common pattern, Spark has a shorthand for\n",
    "doing this efficiently: selectExpr. This is probably the most convenient interface for everyday use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "uG3jk6DV-CMC"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|    newColumnName|      Destination|\n",
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|DEST_COUNTRY_NAME|\n",
      "|    United States|    United States|\n",
      "+-----------------+-----------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.selectExpr(\"Destination as newColumnName\", \"Destination\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sjNcl2u-CMC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6etbrXYH-CMC"
   },
   "source": [
    "This opens up the true power of Spark. We can treat selectExpr as a simple way to build up\n",
    "complex expressions that create new DataFrames. In fact, we can add any valid non-aggregating SQL\n",
    "statement, and as long as the columns resolve, it will be valid! Here’s a simple example that adds a\n",
    "new column withinCountry to our DataFrame that specifies whether the destination and origin are\n",
    "the same:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|      Destination|             Origin|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|        false|\n",
      "|    United States|            Romania|   15|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"*\", \"(Destination=Origin) as withinCountry\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0eJBSgea-CMD"
   },
   "source": [
    "With select expression, we can also specify aggregations over the entire DataFrame by taking\n",
    "advantage of the functions that we have. These look just like what we have been showing so far:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "| avg(count)|\n",
      "+-----------+\n",
      "|1770.765625|\n",
      "+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"avg(count)\").show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------+\n",
      "|count(DISTINCT Destination)|\n",
      "+---------------------------+\n",
      "|                        133|\n",
      "+---------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.selectExpr(\"count(distinct(Destination))\").show(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+\n",
      "|Destination|\n",
      "+-----------+\n",
      "|   Anguilla|\n",
      "|     Russia|\n",
      "+-----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Destination\").distinct().show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"sqlTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+\n",
      "|              dest|\n",
      "+------------------+\n",
      "|            Mexico|\n",
      "|     United States|\n",
      "|     United States|\n",
      "|           Germany|\n",
      "|            Canada|\n",
      "|Dominican Republic|\n",
      "|             Japan|\n",
      "|     United States|\n",
      "|     United States|\n",
      "|    United Kingdom|\n",
      "|     United States|\n",
      "|     United States|\n",
      "|     United States|\n",
      "|       South Korea|\n",
      "+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select Destination as dest from sqlTable where count>1000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mafDp51j-CME"
   },
   "source": [
    "### Converting to Spark Types (Literals)\n",
    "Sometimes, we need to pass explicit values into Spark that are just a value (rather than a new\n",
    "column). This might be a constant value or something we’ll need to compare to later on. The way we\n",
    "do this is through literals. This is basically a translation from a given programming language’s literal\n",
    "value to one that Spark understands. Literals are expressions and you can use them in the same way:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "id": "blR30E-k-CME"
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import lit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "id": "blR30E-k-CME"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+---+\n",
      "|      Destination|  1|\n",
      "+-----------------+---+\n",
      "|DEST_COUNTRY_NAME|  1|\n",
      "|    United States|  1|\n",
      "|    United States|  1|\n",
      "|    United States|  1|\n",
      "|            Egypt|  1|\n",
      "+-----------------+---+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"Destination\", lit(1)).show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "id": "EJporHkN-CME"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---+\n",
      "|      Destination|             Origin|count|One|\n",
      "+-----------------+-------------------+-----+---+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|  1|\n",
      "|    United States|            Romania|   15|  1|\n",
      "|    United States|            Croatia|    1|  1|\n",
      "+-----------------+-------------------+-----+---+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(\"*\", lit(1).alias(\"One\")).show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vUuvIM_9-CMF"
   },
   "source": [
    "### Adding Columns\n",
    "There’s also a more formal way of adding a new column to a DataFrame, and that’s by using the\n",
    "withColumn method on our DataFrame. For example, let’s add a column that just adds the number\n",
    "one as a column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "id": "aLYXKlDl-CMF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+---------+\n",
      "|      Destination|             Origin|count|numberOne|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|        1|\n",
      "|    United States|            Romania|   15|        1|\n",
      "+-----------------+-------------------+-----+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.withColumn(\"numberOne\", lit(1)).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "05VJD1dq-CMF"
   },
   "source": [
    "Let’s do something a bit more interesting and make it an actual expression. In the next example, we’ll\n",
    "set a Boolean flag for when the origin country is the same as the destination country:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "0XHotGGk-CMF"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+-------------+\n",
      "|      Destination|             Origin|count|withinCountry|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|        false|\n",
      "|    United States|            Romania|   15|        false|\n",
      "+-----------------+-------------------+-----+-------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.withColumn(\"withinCountry\", expr(\"Origin == Destination\"))\\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rcl-rgeE-CMF"
   },
   "source": [
    "Notice that the withColumn function takes two arguments: the column name and the expression that\n",
    "will create the value for that given row in the DataFrame. Interestingly, we can also rename a column\n",
    "this way. Although we can rename a column in the manner that we just described, another alternative is to use\n",
    "the withColumnRenamed method. This will rename the column with the name of the string in the first\n",
    "argument to the string in the second argument:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FkemQAdg-CMG"
   },
   "source": [
    "### Removing Columns\n",
    "Now that we’ve created this column, let’s take a look at how we can remove columns from\n",
    "DataFrames. You likely already noticed that we can do this by using select. However, there is also a\n",
    "dedicated method called drop:\n",
    "df.drop(\"ORIGIN_COUNTRY_NAME\").columns\n",
    "We can drop multiple columns by passing in multiple columns as arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfWithLongColName = df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "9sBFuePP-CMG"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Destination', 'Origin', 'count']"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfWithLongColName.drop(\"ORIGIN_COUNTRY_NAME\", \"DEST_COUNTRY_NAME\").columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Destination', 'Origin', 'count']"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ufJyTPO-CMG"
   },
   "source": [
    "### Filtering Rows\n",
    "To filter rows, we create an expression that evaluates to true or false. You then filter out the rows\n",
    "with an expression that is equal to false. The most common way to do this with DataFrames is to\n",
    "create either an expression as a String or build an expression by using a set of column manipulations.\n",
    "There are two methods to perform this operation: you can use where or filter and they both will\n",
    "perform the same operation and accept the same argument types when used with DataFrames. We will\n",
    "stick to where because of its familiarity to SQL; however, filter is valid as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "id": "llNsPsMr-CMG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+-----+\n",
      "|  Destination|       Origin|count|\n",
      "+-------------+-------------+-----+\n",
      "|United States|    Singapore|    1|\n",
      "|      Moldova|United States|    1|\n",
      "+-------------+-------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.where(col(\"count\") < 2).where(col(\"Origin\") != \"Croatia\")\\\n",
    "  .show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fiu67y53-CMH"
   },
   "source": [
    "### Getting Unique Rows\n",
    "A very common use case is to extract the unique or distinct values in a DataFrame. These values can\n",
    "be in one or more columns. The way we do this is by using the distinct method on a DataFrame,\n",
    "which allows us to deduplicate any rows that are in that DataFrame. For instance, let’s get the unique\n",
    "origins in our dataset. This, of course, is a transformation that will return a new DataFrame with only\n",
    "unique rows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "Oqs9Vrku-CMH"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------+--------------------+\n",
      "|          Origin|         Destination|\n",
      "+----------------+--------------------+\n",
      "|         Romania|       United States|\n",
      "|         Croatia|       United States|\n",
      "|         Ireland|       United States|\n",
      "|   United States|               Egypt|\n",
      "|           India|       United States|\n",
      "|       Singapore|       United States|\n",
      "|         Grenada|       United States|\n",
      "|   United States|          Costa Rica|\n",
      "|   United States|             Senegal|\n",
      "|   United States|             Moldova|\n",
      "|    Sint Maarten|       United States|\n",
      "|Marshall Islands|       United States|\n",
      "|   United States|              Guyana|\n",
      "|   United States|               Malta|\n",
      "|   United States|            Anguilla|\n",
      "|   United States|             Bolivia|\n",
      "|        Paraguay|       United States|\n",
      "|   United States|             Algeria|\n",
      "|   United States|Turks and Caicos ...|\n",
      "|       Gibraltar|       United States|\n",
      "+----------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.select(\"Origin\", \"Destination\").distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "24tUhRCT-CMH"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JigkjyFA-CMI"
   },
   "source": [
    "### Random Splits\n",
    "Random splits can be helpful when you need to break up your DataFrame into a random “splits” of\n",
    "the original DataFrame. This is often used with machine learning algorithms to create training,\n",
    "validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "BTtenB-b-CMI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "seed = 5\n",
    "withReplacement = False\n",
    "fraction = 0.5\n",
    "df.sample(withReplacement, fraction, seed).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hx2382hz-CMI"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "bCzcK_zn-CMI"
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "dataFrames = df.randomSplit([0.25, 0.75], seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+-----+\n",
      "|        Destination|       Origin|count|\n",
      "+-------------------+-------------+-----+\n",
      "|           Anguilla|United States|   41|\n",
      "|Antigua and Barbuda|United States|  126|\n",
      "+-------------------+-------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataFrames[1].show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "bCzcK_zn-CMI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataFrames[0].count() > dataFrames[1].count() # False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "olrrJHX8-CMI"
   },
   "source": [
    "### Concatenating and Appending Rows (Union)\n",
    "As you learned in the previous section, DataFrames are immutable. This means users cannot append\n",
    "to DataFrames because that would be changing it. To append to a DataFrame, you must union the\n",
    "original DataFrame along with the new DataFrame. This just concatenates the two DataFramess. To\n",
    "union two DataFrames, you must be sure that they have the same schema and number of columns;\n",
    "otherwise, the union will fail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "gIeyJTPd-CMI"
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql import Row\n",
    "schema = df.schema\n",
    "newRows = [\n",
    "  Row(\"New Country\", \"Other Country\", 5),\n",
    "  Row(\"New Country 2\", \"Other Country 3\", 1)\n",
    "]\n",
    "parallelizedRows = spark.sparkContext.parallelize(newRows)\n",
    "newDF = spark.createDataFrame(parallelizedRows, schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+---------------+-----+\n",
      "|  Destination|         Origin|count|\n",
      "+-------------+---------------+-----+\n",
      "|  New Country|  Other Country|    5|\n",
      "|New Country 2|Other Country 3|    1|\n",
      "+-------------+---------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "newDF.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sh6Vrgt--CMJ"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "id": "UUXKCzQz-CMJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+----------------+-----+\n",
      "|  Destination|          Origin|count|\n",
      "+-------------+----------------+-----+\n",
      "|United States|         Croatia|    1|\n",
      "|United States|       Singapore|    1|\n",
      "|United States|       Gibraltar|    1|\n",
      "|United States|          Cyprus|    1|\n",
      "|United States|         Estonia|    1|\n",
      "|United States|       Lithuania|    1|\n",
      "|United States|        Bulgaria|    1|\n",
      "|United States|         Georgia|    1|\n",
      "|United States|         Bahrain|    1|\n",
      "|United States|Papua New Guinea|    1|\n",
      "|United States|      Montenegro|    1|\n",
      "|United States|         Namibia|    1|\n",
      "|New Country 2| Other Country 3|    1|\n",
      "+-------------+----------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.union(newDF)\\\n",
    "  .where(\"count = 1\")\\\n",
    "  .where(col(\"Origin\") != \"United States\")\\\n",
    "  .show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HDKpYUfe-CMJ"
   },
   "source": [
    "### Sorting Rows\n",
    "When we sort the values in a DataFrame, we always want to sort with either the largest or smallest\n",
    "values at the top of a DataFrame. There are two equivalent operations to do this sort and orderBy\n",
    "that work the exact same way. They accept both column expressions and strings as well as multiple\n",
    "columns. The default is to sort in ascending order:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "id": "Wg0soc4G-CMJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|         Destination|             Origin|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n",
      "|       United States|          Singapore|    1|\n",
      "|               Malta|      United States|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.sort(\"count\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "id": "Wg0soc4G-CMJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|      Destination|             Origin|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n",
      "|     Burkina Faso|      United States|    1|\n",
      "|    Cote d'Ivoire|      United States|    1|\n",
      "|           Cyprus|      United States|    1|\n",
      "|         Djibouti|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(\"count\", \"Destination\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NEb_tmrh-CMJ"
   },
   "source": [
    "To more explicitly specify sort direction, you need to use the asc and desc functions if operating on a\n",
    "column. These allow you to specify the order in which a given column should be sorted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "id": "Z_ViYY2D-CMJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|      Destination|             Origin|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n",
      "|            Malta|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import desc, asc\n",
    "df.orderBy(expr(\"count desc\")).show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "id": "Z_ViYY2D-CMJ"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+-------------+------+\n",
      "|  Destination|       Origin| count|\n",
      "+-------------+-------------+------+\n",
      "|United States|United States|370002|\n",
      "|United States|       Canada|  8483|\n",
      "+-------------+-------------+------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.orderBy(col(\"count\").desc(), col(\"Destination\").asc()).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k4Oa3h7_-CMJ"
   },
   "source": [
    "For optimization purposes, it’s sometimes advisable to sort within each partition before another set of\n",
    "transformations. You can use the sortWithinPartitions method to do this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "id": "m4PXs3PT-CMK"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[DEST_COUNTRY_NAME: string, ORIGIN_COUNTRY_NAME: string, count: bigint]"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "spark.read.format(\"json\").load(\"gs://info323-ya45-spring2023/notebooks/jupyter/*-summary.json\")\\\n",
    "  .sortWithinPartitions(\"count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EwQGPQgp-CMK"
   },
   "source": [
    "### Limit\n",
    "Oftentimes, you might want to restrict what you extract from a DataFrame; for example, you might\n",
    "want just the top ten of some DataFrame. You can do this by using the limit method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "id": "h8Jw6oS9-CMK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|      Destination|             Origin|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "+-----------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.limit(5).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "id": "h8Jw6oS9-CMK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|         Destination|             Origin|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n",
      "|       United States|          Singapore|    1|\n",
      "|Saint Vincent and...|      United States|    1|\n",
      "|               Malta|      United States|    1|\n",
      "|       United States|            Croatia|    1|\n",
      "|       United States|          Gibraltar|    1|\n",
      "+--------------------+-------------------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.orderBy(expr(\"count desc\")).limit(6).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bouealo4-CMK"
   },
   "source": [
    "## Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "id": "u1I8waLe-CMK"
   },
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView('df_table')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "id": "doIZU_tt-CMK"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|      Destination|             Origin|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from df_table\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j8p9hTJv-CML"
   },
   "source": [
    "## To Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "id": "yBuY1ME7-CML"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Destination</th>\n",
       "      <th>Origin</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DEST_COUNTRY_NAME</td>\n",
       "      <td>ORIGIN_COUNTRY_NAME</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>United States</td>\n",
       "      <td>Romania</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>United States</td>\n",
       "      <td>Croatia</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>United States</td>\n",
       "      <td>Ireland</td>\n",
       "      <td>344.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Egypt</td>\n",
       "      <td>United States</td>\n",
       "      <td>15.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>252</th>\n",
       "      <td>United States</td>\n",
       "      <td>Saint Kitts and Nevis</td>\n",
       "      <td>145.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>253</th>\n",
       "      <td>Uruguay</td>\n",
       "      <td>United States</td>\n",
       "      <td>43.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>254</th>\n",
       "      <td>United States</td>\n",
       "      <td>Haiti</td>\n",
       "      <td>225.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>Bonaire, Sint Eustatius, and Saba</td>\n",
       "      <td>United States</td>\n",
       "      <td>58.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>Greece</td>\n",
       "      <td>United States</td>\n",
       "      <td>30.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>257 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Destination                 Origin  count\n",
       "0                    DEST_COUNTRY_NAME    ORIGIN_COUNTRY_NAME    NaN\n",
       "1                        United States                Romania   15.0\n",
       "2                        United States                Croatia    1.0\n",
       "3                        United States                Ireland  344.0\n",
       "4                                Egypt          United States   15.0\n",
       "..                                 ...                    ...    ...\n",
       "252                      United States  Saint Kitts and Nevis  145.0\n",
       "253                            Uruguay          United States   43.0\n",
       "254                      United States                  Haiti  225.0\n",
       "255  Bonaire, Sint Eustatius, and Saba          United States   58.0\n",
       "256                             Greece          United States   30.0\n",
       "\n",
       "[257 rows x 3 columns]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas() #Why and when?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd = df.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "id": "tt0cHGVa-CML"
   },
   "outputs": [],
   "source": [
    "df_sp = spark.createDataFrame(df_pd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "id": "gEGGPQB--CML"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+-------------------+-----+\n",
      "|         Destination|             Origin|count|\n",
      "+--------------------+-------------------+-----+\n",
      "|   DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|  NaN|\n",
      "|       United States|            Romania| 15.0|\n",
      "|       United States|            Croatia|  1.0|\n",
      "|       United States|            Ireland|344.0|\n",
      "|               Egypt|      United States| 15.0|\n",
      "|       United States|              India| 62.0|\n",
      "|       United States|          Singapore|  1.0|\n",
      "|       United States|            Grenada| 62.0|\n",
      "|          Costa Rica|      United States|588.0|\n",
      "|             Senegal|      United States| 40.0|\n",
      "|             Moldova|      United States|  1.0|\n",
      "|       United States|       Sint Maarten|325.0|\n",
      "|       United States|   Marshall Islands| 39.0|\n",
      "|              Guyana|      United States| 64.0|\n",
      "|               Malta|      United States|  1.0|\n",
      "|            Anguilla|      United States| 41.0|\n",
      "|             Bolivia|      United States| 30.0|\n",
      "|       United States|           Paraguay|  6.0|\n",
      "|             Algeria|      United States|  4.0|\n",
      "|Turks and Caicos ...|      United States|230.0|\n",
      "+--------------------+-------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_sp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z5V3b8tY-CML"
   },
   "source": [
    "### Repartition and Coalesce\n",
    "Another important optimization opportunity is to partition the data according to some frequently\n",
    "filtered columns, which control the physical layout of data across the cluster including the partitioning\n",
    "scheme and the number of partitions.\n",
    "Repartition will incur a full shuffle of the data, regardless of whether one is necessary. This means\n",
    "that you should typically only repartition when the future number of partitions is greater than your\n",
    "current number of partitions or when you are looking to partition by a set of columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "id": "OxYrN18X-CMM"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.rdd.getNumPartitions() # 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrIypwUT-CMM"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "id": "1M4uEUgP-CMQ"
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df_5 =df.repartition(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_5.rdd.getNumPartitions()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sDWlRuRm-CMQ"
   },
   "source": [
    "If you know that you’re going to be filtering by a certain column often, it can be worth repartitioning\n",
    "based on that column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "id": "ejMxizk4-CMQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Destination: string, Origin: string, count: bigint]"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.repartition(col(\"Destination\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Az4HZ-0R-CMQ"
   },
   "source": [
    "You can optionally specify the number of partitions you would like, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "id": "MqfFain5-CMQ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Destination: string, Origin: string, count: bigint]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.repartition(5, col(\"Destination\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCKWxLVI-CMR"
   },
   "source": [
    "Coalesce, on the other hand, will not incur a full shuffle and will try to combine partitions. This\n",
    "operation will shuffle your data into five partitions based on the destination country name, and then\n",
    "coalesce them (without a full shuffle):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "id": "BfSrZ-0K-CMR"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[Destination: string, Origin: string, count: bigint]"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.repartition(5, col(\"Destination\")).coalesce(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nc9Tmc6Q-CMR"
   },
   "source": [
    "### Collecting Rows to the Driver\n",
    "As discussed in previous chapters, Spark maintains the state of the cluster in the driver. There are\n",
    "times when you’ll want to collect some of your data to the driver in order to manipulate it on your\n",
    "local machine.\n",
    "Thus far, we did not explicitly define this operation. However, we used several different methods for\n",
    "doing so that are effectively all the same. collect gets all data from the entire DataFrame, take\n",
    "selects the first N rows, and show prints out a number of rows nicely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "id": "qtSyDivo-CMS"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|      Destination|             Origin|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME| null|\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "+-----------------+-------------------+-----+\n",
      "\n",
      "+-----------------+-------------------+-----+\n",
      "|Destination      |Origin             |count|\n",
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|null |\n",
      "|United States    |Romania            |15   |\n",
      "|United States    |Croatia            |1    |\n",
      "|United States    |Ireland            |344  |\n",
      "|Egypt            |United States      |15   |\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Row(Destination='DEST_COUNTRY_NAME', Origin='ORIGIN_COUNTRY_NAME', count=None),\n",
       " Row(Destination='United States', Origin='Romania', count=15),\n",
       " Row(Destination='United States', Origin='Croatia', count=1),\n",
       " Row(Destination='United States', Origin='Ireland', count=344),\n",
       " Row(Destination='Egypt', Origin='United States', count=15),\n",
       " Row(Destination='United States', Origin='India', count=62),\n",
       " Row(Destination='United States', Origin='Singapore', count=1),\n",
       " Row(Destination='United States', Origin='Grenada', count=62),\n",
       " Row(Destination='Costa Rica', Origin='United States', count=588),\n",
       " Row(Destination='Senegal', Origin='United States', count=40)]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Exception happened during processing of request from ('127.0.0.1', 33824)\n",
      "Traceback (most recent call last):\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/socketserver.py\", line 316, in _handle_request_noblock\n",
      "    self.process_request(request, client_address)\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/socketserver.py\", line 347, in process_request\n",
      "    self.finish_request(request, client_address)\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/socketserver.py\", line 360, in finish_request\n",
      "    self.RequestHandlerClass(request, client_address, self)\n",
      "  File \"/opt/conda/miniconda3/lib/python3.8/socketserver.py\", line 747, in __init__\n",
      "    self.handle()\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 262, in handle\n",
      "    poll(accum_updates)\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 235, in poll\n",
      "    if func():\n",
      "  File \"/usr/lib/spark/python/pyspark/accumulators.py\", line 239, in accum_updates\n",
      "    num_updates = read_int(self.rfile)\n",
      "  File \"/usr/lib/spark/python/pyspark/serializers.py\", line 564, in read_int\n",
      "    raise EOFError\n",
      "EOFError\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "collectDF = df.limit(10)\n",
    "collectDF.take(5) # take works with an Integer count\n",
    "collectDF.show() # this prints it out nicely\n",
    "collectDF.show(5, False)\n",
    "collectDF.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-0jk-nQ3-CMS"
   },
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "INFO323-Lecture-presented-week5-Spark-4-Structured-API.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}