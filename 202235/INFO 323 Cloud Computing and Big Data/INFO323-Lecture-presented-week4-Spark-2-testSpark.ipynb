{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFO 323: Cloud Computing and Big Data\n",
    "## CCI at Drexel University\n",
    "### Yuan An"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check the Apache Spark Availability and Version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataFrame and RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.range(10).toDF(\"id\").rdd.map(lambda row: row[0]).collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Example: Word Count: Access Data from Google File System\n",
    "### 1. We use sparkContext object to read a text file as records of Resilient Distributed Dataset (RDD). Each line is a record. RDD is the primitive data structure used by Spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = spark.sparkContext.textFile(\"gs://info323-ya45-bucket/notebooks/jupyter/words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg EBook of A Tale of Two Cities, by Charles Dickens',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " 'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
       " 're-use it under the terms of the Project Gutenberg License included']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transform the RDD into records of individual words\n",
    "We will use the flatMap() operation to convert the records of lines to records of individual words. \n",
    "\n",
    "Mapping is a basic operation for transforming RDD records. We specify a function\n",
    "that returns the value that we want, given the correct input. We then apply that, record by record.\n",
    "\n",
    "Because we want the words from all lines reside in the same list, not in nested lists, we apply flatMap() which flatten nested lists.\n",
    "\n",
    "Compare the results of map() and flatMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = file.map(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'Project',\n",
       "  'Gutenberg',\n",
       "  'EBook',\n",
       "  'of',\n",
       "  'A',\n",
       "  'Tale',\n",
       "  'of',\n",
       "  'Two',\n",
       "  'Cities,',\n",
       "  'by',\n",
       "  'Charles',\n",
       "  'Dickens'],\n",
       " [],\n",
       " ['This',\n",
       "  'eBook',\n",
       "  'is',\n",
       "  'for',\n",
       "  'the',\n",
       "  'use',\n",
       "  'of',\n",
       "  'anyone',\n",
       "  'anywhere',\n",
       "  'at',\n",
       "  'no',\n",
       "  'cost',\n",
       "  'and',\n",
       "  'with']]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = file.flatMap(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Project', 'Gutenberg']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Transform the records of individual words to records of word-count_1 pairs\n",
    "We apply the map() operation to the records of words and output word-1 pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_1 = words.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 1), ('Project', 1), ('Gutenberg', 1), ('EBook', 1), ('of', 1)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_1.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Reduce the records of word_1 pairs to records of word_count pairs\n",
    "\n",
    "We can use the reduce() method to specify a function to “reduce” an RDD of any kind of value to one\n",
    "value. For instance, given a set of numbers, we can reduce this to its sum by specifying a function that\n",
    "takes as input two values and reduces them into one. \n",
    "\n",
    "For the word count application, we use reduceByKey() to count different words.\n",
    "\n",
    "The reduce() method is an action which kicks off the specified transformations. It returns the final result.\n",
    "\n",
    "However, reduceByKey() is a transformation which transforms one RDD to another RDD. To kick off the transformations, an action is required such as collect() or take().\n",
    "\n",
    "Compare the results of reduce() and reduceByKey()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_total = word_1.reduce(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The', 1, 'Project', 1, 'Gutenberg')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_total[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = word_1.reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 587), ('Project', 78), ('EBook', 2), ('of', 4065), ('Tale', 3)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save the results which may be in multiple partitions\n",
    "Specify a new folder to save the results. If the folder name exists, error messages may appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.saveAsTextFile(\"gs://info323-ya45-bucket/notebooks/jupyter/counts-spark\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 6. Go back to the storage to download the partitions and assemble the results\n",
    "You may need to download the partitions to a local system and combine the partitions to a single file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark Example: Word Count: Access Data from HDFS\n",
    "### 1. We use sparkContext object to read a text file as records of Resilient Distributed Dataset (RDD). Each line is a record. RDD is the primitive data structure used by Spark. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "file = spark.sparkContext.textFile(\"hdfs:/user/words/words.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.rdd.RDD"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The Project Gutenberg EBook of A Tale of Two Cities, by Charles Dickens',\n",
       " '',\n",
       " 'This eBook is for the use of anyone anywhere at no cost and with',\n",
       " 'almost no restrictions whatsoever.  You may copy it, give it away or',\n",
       " 're-use it under the terms of the Project Gutenberg License included']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Transform the RDD into records of individual words\n",
    "We will use the flatMap() operation to convert the records of lines to records of individual words. \n",
    "\n",
    "Mapping is a basic operation for transforming RDD records. We specify a function\n",
    "that returns the value that we want, given the correct input. We then apply that, record by record.\n",
    "\n",
    "Because we want the words from all lines reside in the same list, not in nested lists, we apply flatMap() which flatten nested lists.\n",
    "\n",
    "Compare the results of map() and flatMap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = file.map(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['The',\n",
       "  'Project',\n",
       "  'Gutenberg',\n",
       "  'EBook',\n",
       "  'of',\n",
       "  'A',\n",
       "  'Tale',\n",
       "  'of',\n",
       "  'Two',\n",
       "  'Cities,',\n",
       "  'by',\n",
       "  'Charles',\n",
       "  'Dickens'],\n",
       " [],\n",
       " ['This',\n",
       "  'eBook',\n",
       "  'is',\n",
       "  'for',\n",
       "  'the',\n",
       "  'use',\n",
       "  'of',\n",
       "  'anyone',\n",
       "  'anywhere',\n",
       "  'at',\n",
       "  'no',\n",
       "  'cost',\n",
       "  'and',\n",
       "  'with']]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "words = file.flatMap(lambda line: line.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Project', 'Gutenberg']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Transform the records of individual words to records of word-count_1 pairs\n",
    "We apply the map() operation to the records of words and output word-1 pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_1 = words.map(lambda word: (word, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 1), ('Project', 1), ('Gutenberg', 1), ('EBook', 1), ('of', 1)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_1.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Reduce the records of word_1 pairs to records of word_count pairs\n",
    "\n",
    "We can use the reduce() method to specify a function to “reduce” an RDD of any kind of value to one\n",
    "value. For instance, given a set of numbers, we can reduce this to its sum by specifying a function that\n",
    "takes as input two values and reduces them into one. \n",
    "\n",
    "For the word count application, we use reduceByKey() to count different words.\n",
    "\n",
    "The reduce() method is an action which kicks off the specified transformations. It returns the final result.\n",
    "\n",
    "However, reduceByKey() is a transformation which transforms one RDD to another RDD. To kick off the transformations, an action is required such as collect() or take().\n",
    "\n",
    "Compare the results of reduce() and reduceByKey()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_total = word_1.reduce(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('The', 1, 'Project', 1, 'Gutenberg')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts_total[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = word_1.reduceByKey(lambda a, b: a+b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 587), ('Project', 78), ('EBook', 2), ('of', 4065), ('Tale', 3)]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "counts.take(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Save the results which may be in multiple partitions\n",
    "Specify a new folder to save the results. If the folder name exists, error messages may appear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts.saveAsTextFile(\"hdfs:/user/word-counts\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "source": [
    "### 6. Go back to the storage to download the partitions and assemble the results\n",
    "You may need to download the partitions to a local system and combine the partitions to a single file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pi: Compute an approximate value of $\\pi$\n",
    "The area of square of side 2 is 4. The area of the circle inside the square is $\\pi \\times 1^2 = \\pi$. So $\\frac{\\pi}{4}\\approx \\frac{\\textrm{no. of points generated inside the circle}}{\\textrm{no. of points generated inside the square}} $.\n",
    "\n",
    "$\\pi \\approx 4 \\times \\frac{\\textrm{no. of points generated inside the circle}}{\\textrm{no. of points generated inside the square}}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Define a utility function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inside(p):\n",
    "    x, y = random.random(), random.random()\n",
    "    return x**2 + y**2 < 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Compute the number of points in the circle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "785211\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "NUM_SAMPLES = 1000000\n",
    "count = sc.parallelize(range(0, NUM_SAMPLES)).filter(inside).count()\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Approximate $\\pi$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pi is roughly 3.140844\n"
     ]
    }
   ],
   "source": [
    "print(\"Pi is roughly %f\" % (4.0 * count / NUM_SAMPLES))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}