{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\"> INFO 323: Cloud Computing and Big Data</h1>\n",
    "<h2 style=\"text-align:center\"> College of Computing and Informatics</h2>\n",
    "<h2 style=\"text-align:center\">Drexel University</h2>\n",
    "\n",
    "<h3 style=\"text-align:center\"> Introduction to Spark Programming</h3>\n",
    "<h3 style=\"text-align:center\"> Yuan An, PhD</h3>\n",
    "<h3 style=\"text-align:center\">Associate Professor</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Computing Recap\n",
    "- Typically, when we think of a “computer,” we think about one machine sitting on our desk at home or at work.\n",
    "- There are some things that our computer is not powerful enough to perform. One particularly challenging area is data processing.\n",
    "- Single machines do not have enough power and resources to perform computations on huge amounts of information (or the user probably does not have the time to wait for the computation to finish). \n",
    "- A cluster, or group, of computers, pools the resources of many machines together, giving us the ability to use all the cumulative resources as if they were a single computer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Computing and Spark\n",
    "- Now, a group of machines alone is not powerful, we need a framework to coordinate work across them. \n",
    "- Spark does just that, managing and coordinating the execution of tasks on data across a cluster of computers.\n",
    "- The cluster of machines that Spark will use to execute tasks is managed by a cluster manager like\n",
    " - * Spark’s standalone cluster manager, \n",
    " - * YARN, \n",
    " - * or Mesos. \n",
    "- We then submit Spark Applications to these cluster managers, which will grant resources to our application so that we can complete our work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Cluster\n",
    "Spark cluster consists of driver and worker nodes.\n",
    "![](https://i.imgur.com/6I4l6ZH.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Applications\n",
    "Spark Applications consist of a driver process and a set of executor processes. \n",
    "![](https://i.imgur.com/zF7Ngfc.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Executor\n",
    "Each executor’s core gets a partition of data to work on\n",
    "![](https://i.imgur.com/FZN5dCb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Driver\n",
    "- Each Spark driver creates one or more Spark jobs; \n",
    "- Each Spark job creates one or more stages; \n",
    "- Each Spark stage creates one or more tasks to be distributed to executors\n",
    "![](https://i.imgur.com/lMTRd1c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Programs\n",
    "A program consists of a sequence of transformations followed by an action.\n",
    "![](https://i.imgur.com/NLTWLo0.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark RDD\n",
    "- The primary data abstraction structure for Spark applications, is one of the main differentiators between Spark and other cluster computing frameworks. \n",
    "- In-memory collections of data distributed across a cluster. \n",
    "- Spark programs using the Spark core API consist of\n",
    "- * loading input data into an RDD\n",
    "- * transforming the RDD into subsequent RDDs\n",
    "- * storing or presenting the final output for an application from the resulting final RDD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames\n",
    "The most common Structured API and simply represents a table of data with rows and columns\n",
    "![](https://i.imgur.com/qvdB1rT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "- In Spark, the core data structures are immutable, meaning they cannot be changed after they’re created. \n",
    "- This might seem like a strange concept at first: if we cannot change it, how are we supposed to use it? \n",
    "- To “change” a DataFrame, we need to instruct Spark how we would like to modify it to do what we want.\n",
    "- These instructions are called transformations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narrow Transformation\n",
    "Transformations consisting of narrow dependencies (we’ll call them narrow transformations) are\n",
    "those for which each input partition will contribute to only one output partition.\n",
    "# Wide Transformation\n",
    "A wide dependency (or wide transformation) style transformation will have input partitions\n",
    "contributing to many output partitions. You will often hear this referred to as a shuffle whereby Spark\n",
    "will exchange partitions across the cluster. With narrow transformations, Spark will automatically\n",
    "perform an operation called pipelining, meaning that if we specify multiple filters on DataFrames,\n",
    "they’ll all be performed in-memory. The same cannot be said for shuffles. When we perform a shuffle,\n",
    "Spark writes the results to disk. \n",
    "![narrow vs wide](https://i.imgur.com/jJ4fypS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Evaluation\n",
    "Lazy evaluation means that Spark will wait until the very last moment to execute the graph of\n",
    "computation instructions. \n",
    "\n",
    "In Spark, instead of modifying the data immediately when you express some\n",
    "operation, you build up a plan of transformations that you would like to apply to your source data. \n",
    "\n",
    "By waiting until the last minute to execute the code, Spark compiles this plan from your raw DataFrame\n",
    "transformations to a streamlined physical plan that will run as efficiently as possible across the\n",
    "cluster. \n",
    "\n",
    "This provides immense benefits because Spark can optimize the entire data flow from end to\n",
    "end. \n",
    "\n",
    "An example of this is something called predicate pushdown on DataFrames. If we build a large\n",
    "Spark job but specify a filter at the end that only requires us to fetch one row from our source data,\n",
    "the most efficient way to execute this is to access the single record that we need. Spark will actually\n",
    "optimize this for us by pushing the filter down automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions\n",
    "- To trigger the computation, we run an action - instructs Spark to compute a result from a series of transformations. \n",
    "- There are three kinds of actions:\n",
    "- * Actions to view data in the console\n",
    "- * Actions to collect data to native objects in the respective language\n",
    "- * Actions to write to output data sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Example: Counting MnM for the Cookie Monster\n",
    "\n",
    "Let’s solve problem, but with a larger data set and using more of Spark’s distribution\n",
    "functionality and DataFrame APIs. We will cover the APIs used in this program later.\n",
    "\n",
    "Let’s write a Spark program that reads a file with over 100,000 entries (where each\n",
    "row or line has a <state, mnm_color, count>) and computes and aggregates the\n",
    "counts for each color and state. These aggregated counts tell us the colors of M&Ms\n",
    "favored by students in each state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.4.8'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Get the M&M data set filename from the command-line arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Change the following path to your bucket\n",
    "mnm_file = \"gs://info323-ya45-spring2023/notebooks/jupyter/mnm_dataset.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Read the file into a Spark DataFrame using the CSV format by inferring the schema and specifying that the file contains a header, which provides column names for comma-separated fields."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnm_df = (spark.read.format(\"csv\")\n",
    ".option(\"header\", \"true\")\n",
    ".option(\"inferSchema\", \"true\")\n",
    ".load(mnm_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StructType(List(StructField(State,StringType,true),StructField(Color,StringType,true),StructField(Count,IntegerType,true)))\n"
     ]
    }
   ],
   "source": [
    "print(mnm_df.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- State: string (nullable = true)\n",
      " |-- Color: string (nullable = true)\n",
      " |-- Count: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mnm_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. We use the DataFrame high-level APIs. \n",
    "1. Select from the DataFrame the fields \"State\", \"Color\", and \"Count\"\n",
    "2. Since we want to group each state and its M&M color count, we use groupBy()\n",
    "3. Aggregate counts of all colors and groupBy() State and Color\n",
    "4. orderBy() in descending order\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import count\n",
    "\n",
    "count_mnm_df = (mnm_df\n",
    ".select(\"State\", \"Color\", \"Count\")\n",
    ".groupBy(\"State\", \"Color\")\n",
    ".agg(count(\"Count\").alias(\"Total\"))\n",
    ".orderBy(\"Total\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Show the resulting aggregations for all the states and colors; a total count of each color per state. Note show() is an action, which will trigger the above query to be executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State|Color |Total|\n",
      "+-----+------+-----+\n",
      "|CA   |Yellow|1807 |\n",
      "|WA   |Green |1779 |\n",
      "|OR   |Orange|1743 |\n",
      "|TX   |Green |1737 |\n",
      "|TX   |Red   |1725 |\n",
      "|CA   |Green |1723 |\n",
      "|CO   |Yellow|1721 |\n",
      "|CA   |Brown |1718 |\n",
      "|CO   |Green |1713 |\n",
      "|NV   |Orange|1712 |\n",
      "|TX   |Yellow|1703 |\n",
      "|NV   |Green |1698 |\n",
      "|AZ   |Brown |1698 |\n",
      "|WY   |Green |1695 |\n",
      "|CO   |Blue  |1695 |\n",
      "|NM   |Red   |1690 |\n",
      "|AZ   |Orange|1689 |\n",
      "|NM   |Yellow|1688 |\n",
      "|NM   |Brown |1687 |\n",
      "|UT   |Orange|1684 |\n",
      "|NM   |Green |1682 |\n",
      "|UT   |Red   |1680 |\n",
      "|AZ   |Green |1676 |\n",
      "|NV   |Yellow|1675 |\n",
      "|NV   |Blue  |1673 |\n",
      "|WA   |Red   |1671 |\n",
      "|WY   |Red   |1670 |\n",
      "|WA   |Brown |1669 |\n",
      "|NM   |Orange|1665 |\n",
      "|WY   |Blue  |1664 |\n",
      "|WA   |Yellow|1663 |\n",
      "|WA   |Orange|1658 |\n",
      "|CA   |Orange|1657 |\n",
      "|NV   |Brown |1657 |\n",
      "|CO   |Brown |1656 |\n",
      "|CA   |Red   |1656 |\n",
      "|UT   |Blue  |1655 |\n",
      "|AZ   |Yellow|1654 |\n",
      "|TX   |Orange|1652 |\n",
      "|AZ   |Red   |1648 |\n",
      "|OR   |Blue  |1646 |\n",
      "|OR   |Red   |1645 |\n",
      "|UT   |Yellow|1645 |\n",
      "|CO   |Orange|1642 |\n",
      "|TX   |Brown |1641 |\n",
      "|NM   |Blue  |1638 |\n",
      "|AZ   |Blue  |1636 |\n",
      "|OR   |Green |1634 |\n",
      "|UT   |Brown |1631 |\n",
      "|WY   |Yellow|1626 |\n",
      "|WA   |Blue  |1625 |\n",
      "|CO   |Red   |1624 |\n",
      "|OR   |Brown |1621 |\n",
      "|TX   |Blue  |1614 |\n",
      "|OR   |Yellow|1614 |\n",
      "|NV   |Red   |1610 |\n",
      "|CA   |Blue  |1603 |\n",
      "|WY   |Orange|1595 |\n",
      "|UT   |Green |1591 |\n",
      "|WY   |Brown |1532 |\n",
      "+-----+------+-----+\n",
      "\n",
      "Total Rows = 60\n"
     ]
    }
   ],
   "source": [
    "count_mnm_df.show(n=60, truncate=False)\n",
    "print(\"Total Rows = %d\" % (count_mnm_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. While the above code aggregated and counted for all the states, what if we just want to see the data for a single state, e.g., CA?\n",
    "1. Select from all rows in the DataFrame\n",
    "2. Filter only CA state\n",
    "3. groupBy() State and Color as we did above\n",
    "4. Aggregate the counts for each color\n",
    "5. orderBy() in descending order\n",
    "6. Find the aggregate count for California by filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "ca_count_mnm_df = (mnm_df\n",
    ".select(\"State\", \"Color\", \"Count\")\n",
    ".where(mnm_df.State == \"CA\")\n",
    ".groupBy(\"State\", \"Color\")\n",
    ".agg(count(\"Count\").alias(\"Total\"))\n",
    ".orderBy(\"Total\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show the resulting aggregation for California. As above, show() is an action that will trigger the execution of the entire computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------+-----+\n",
      "|State|Color |Total|\n",
      "+-----+------+-----+\n",
      "|CA   |Yellow|1807 |\n",
      "|CA   |Green |1723 |\n",
      "|CA   |Brown |1718 |\n",
      "|CA   |Orange|1657 |\n",
      "|CA   |Red   |1656 |\n",
      "|CA   |Blue  |1603 |\n",
      "+-----+------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ca_count_mnm_df.show(n=10, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5311b00f-669b-44d3-ae76-a6b8bb619cfa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## An End-to-End Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a3f9fc0f-e1b1-42ad-851a-9784dc3d0dcd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Spark reads in a DataFrame from a file. The DataFrame has a set of columns with an unspecified number of rows. The reason the number of rows is unspecified is because reading data is a transformation, and\n",
    "is therefore a lazy operation. Spark peeked at only a couple of rows of data to try to guess what types\n",
    "each column should be.\n",
    "![](https://i.imgur.com/o1y1JzQ.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4f51a7ef-d969-4b09-87cd-3ff9d589f62a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file=\"gs://info323-ya45-spring2023/notebooks/jupyter/2015-summary.csv\"\n",
    "flightdata = spark.read.option(\"inferSchema\", \"true\").option(\"header\", \"true\").csv(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b79ea761-c68e-4f50-acd1-196665cb9d41",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Romania', count=15),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Croatia', count=1),\n",
       " Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Ireland', count=344)]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightdata.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3510d856-c825-4254-b80e-fe5bd2643373",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let us specify a wide transformation, sort(). Nothing happens to the data when we call sort because it’s just a transformation. However, we can\n",
    "see that Spark is building up a plan for how it will execute this across the cluster by looking at the\n",
    "explain plan. We can call explain on any DataFrame object to see the DataFrame’s lineage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fb3eee3f-d5a6-4114-b753-1d497992f18f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) Sort [count#12 ASC NULLS FIRST], true, 0\n",
      "+- Exchange rangepartitioning(count#12 ASC NULLS FIRST, 200)\n",
      "   +- *(1) FileScan csv [DEST_COUNTRY_NAME#10,ORIGIN_COUNTRY_NAME#11,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[gs://info323-ya45-spring2023/notebooks/jupyter/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,ORIGIN_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "flightdata.sort(\"count\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e1325899-abcb-4de0-a522-abb35dbad40d",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now we have a sequence of transofrmations as: narror (read) -> wide (sort)\n",
    "\n",
    "You can read explain plans from top to bottom, the top being the\n",
    "end result, and the bottom being the source(s) of data. In this case, take a look at the first keywords.\n",
    "You will see sort, exchange, and FileScan. That’s because the sort of our data is actually a wide\n",
    "transformation because rows will need to be compared with one another. Don’t worry too much about\n",
    "understanding everything about explain plans at this point, they can just be helpful tools for debugging\n",
    "and improving your knowledge as you progress with Spark.\n",
    "\n",
    "Next, we can specify an action to kick off this plan. However, before doing\n",
    "that, we’re going to set a configuration. By default, when we perform a shuffle, Spark outputs 200\n",
    "shuffle partitions. Let’s set this value to 5 to reduce the number of the output partitions from the\n",
    "shuffle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "64fde6d8-bef1-4643-8212-c2ae99804e34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d196c898-5c57-44f5-b68e-6840aaab06d0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(DEST_COUNTRY_NAME='United States', ORIGIN_COUNTRY_NAME='Singapore', count=1),\n",
       " Row(DEST_COUNTRY_NAME='Moldova', ORIGIN_COUNTRY_NAME='United States', count=1)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flightdata.sort(\"count\").take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bad66b27-92df-4c30-8d9e-0a920d41c44e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## DataFrame and SQL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f4232764-894d-4b00-bbf9-3f6365239155",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We worked through a simple transformation in the previous example, let’s now work through a more\n",
    "complex one and follow along in both DataFrames and SQL. Spark can run the same transformations,\n",
    "regardless of the language, in the exact same way. You can express your business logic in SQL or\n",
    "DataFrames (either in R, Python, Scala, or Java) and Spark will compile that logic down to an\n",
    "underlying plan (that you can see in the explain plan) before actually executing your code. With Spark\n",
    "SQL, you can register any DataFrame as a table or view (a temporary table) and query it using pure\n",
    "SQL. There is no performance difference between writing SQL queries or writing DataFrame code,\n",
    "they both “compile” to the same underlying plan that we specify in DataFrame code.\n",
    "You can make any DataFrame into a table or view with one simple method call:\n",
    "![](https://i.imgur.com/uFU3s01.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "68a472a9-1156-43a2-aafe-a329b1b69d07",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "flightdata.createOrReplaceTempView(\"flight_data_table\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "f8b58148-13d9-497c-aee5-047241de2776",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can query our data in SQL. To do so, we’ll use the spark.sql function (remember, spark is our SparkSession variable) that conveniently returns a new DataFrame. Although this might seem a bit circular in logic—that a SQL query against a DataFrame returns another DataFrame—it’s actually\n",
    "quite powerful. This makes it possible for you to specify transformations in the manner most convenient to you at any given point in time and not sacrifice any efficiency to do so! To understand\n",
    "that this is happening, let’s take a look at two explain plans:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ea996169-eef5-4e79-b18f-2204a0835e51",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 5)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [DEST_COUNTRY_NAME#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[gs://info323-ya45-spring2023/notebooks/jupyter/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    }
   ],
   "source": [
    "flightdata.groupBy(\"DEST_COUNTRY_NAME\").count().explain()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ffe24d3a-3cf8-4ad8-88a2-ebe2f8855c44",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "sqlWay = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, count(1)\n",
    "FROM flight_data_table\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "9b9b4abe-80f1-4f53-8ebc-2bdc8ea727f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "*(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[count(1)])\n",
      "+- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 5)\n",
      "   +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_count(1)])\n",
      "      +- *(1) FileScan csv [DEST_COUNTRY_NAME#10] Batched: false, Format: CSV, Location: InMemoryFileIndex[gs://info323-ya45-spring2023/notebooks/jupyter/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string>\n"
     ]
    }
   ],
   "source": [
    "sqlWay.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad2e85a0-6f86-44f9-8474-850f0ef4872c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let’s pull out some interesting statistics from our data. One thing to understand is that DataFrames\n",
    "(and SQL) in Spark already have a huge number of manipulations available. There are hundreds of\n",
    "functions that you can use and import to help you resolve your big data problems faster. We will use\n",
    "the max function, to establish the maximum number of flights to and from any given location. This just\n",
    "scans each value in the relevant column in the DataFrame and checks whether it’s greater than the\n",
    "previous values that have been seen. This is a transformation, because we are effectively filtering\n",
    "down to one row. Let’s see what that looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7688a4b7-2f0b-4bbc-a271-bb7e3166e65d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(max(count)=370002)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"SELECT max(count) FROM flight_data_table\").take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0711f002-c4c7-4a87-9920-a479a50717e7",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Let’s perform something a bit more\n",
    "complicated and find the top five destination countries in the data. This is our first multitransformation\n",
    "query, so we’ll take it step by step. Let’s begin with a fairly straightforward SQL\n",
    "aggregation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "23770464-7942-41d1-bcda-df1f650eb699",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "maxsql = spark.sql(\"\"\"\n",
    "SELECT DEST_COUNTRY_NAME, sum(count) as destination_total\n",
    "FROM flight_data_table\n",
    "GROUP BY DEST_COUNTRY_NAME\n",
    "ORDER BY sum(count) DESC\n",
    "LIMIT 5\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3b716509-f71e-479d-9634-fb9f35581bf0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "maxsql.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8ebd36a4-a853-494d-8635-1b7fb573a899",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now, let’s move to the DataFrame syntax that is semantically similar but slightly different in\n",
    "implementation and ordering. But, as we mentioned, the underlying plans for both of them are the\n",
    "same. Let’s run the queries and see their results as a sanity check:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ba10f3f4-f0e6-4218-88d8-b84f76921955",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3ec7ed4b-9047-4b8d-8236-1406e0abfbd7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-----------------+\n",
      "|DEST_COUNTRY_NAME|destination_total|\n",
      "+-----------------+-----------------+\n",
      "|    United States|           411352|\n",
      "|           Canada|             8399|\n",
      "|           Mexico|             7140|\n",
      "|   United Kingdom|             2025|\n",
      "|            Japan|             1548|\n",
      "+-----------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "flightdata.groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".sum(\"count\").withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    ".sort(desc(\"destination_total\")).limit(5).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "66b55a55-bffa-4a38-bea8-3bc842efdb03",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "The above sequence of transformations has 7 transformation steps: read->groupBy->sum->withColumnRenamed->sort->limit->collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "abff0223-2d6f-414f-a81f-cc0ac2db681d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Physical Plan ==\n",
      "TakeOrderedAndProject(limit=5, orderBy=[destination_total#77L DESC NULLS LAST], output=[DEST_COUNTRY_NAME#10,destination_total#77L])\n",
      "+- *(2) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[sum(cast(count#12 as bigint))])\n",
      "   +- Exchange hashpartitioning(DEST_COUNTRY_NAME#10, 5)\n",
      "      +- *(1) HashAggregate(keys=[DEST_COUNTRY_NAME#10], functions=[partial_sum(cast(count#12 as bigint))])\n",
      "         +- *(1) FileScan csv [DEST_COUNTRY_NAME#10,count#12] Batched: false, Format: CSV, Location: InMemoryFileIndex[gs://info323-ya45-spring2023/notebooks/jupyter/2015-summary.csv], PartitionFilters: [], PushedFilters: [], ReadSchema: struct<DEST_COUNTRY_NAME:string,count:int>\n"
     ]
    }
   ],
   "source": [
    "flightdata.groupBy(\"DEST_COUNTRY_NAME\")\\\n",
    ".sum(\"count\").withColumnRenamed(\"sum(count)\", \"destination_total\")\\\n",
    ".sort(desc(\"destination_total\")).limit(5).explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3473cdae-857b-4ae8-8699-e3c585cae8bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Reader & Writer\n",
    "1. Read from CSV files\n",
    "1. Read from JSON files\n",
    "1. Write DataFrame to files\n",
    "1. Write DataFrame to tables\n",
    "\n",
    "##### Methods\n",
    "- DataFrameReader (<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dataframereader#pyspark.sql.DataFrameReader\" target=\"_blank\">Python</a>/<a href=\"http://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameReader.html\" target=\"_blank\">Scala</a>): `csv`, `json`, `option`, `schema`\n",
    "- DataFrameWriter (<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dataframereader#pyspark.sql.DataFrameWriter\" target=\"_blank\">Python</a>/<a href=\"http://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameWriter.html\" target=\"_blank\">Scala</a>): `mode`, `option`, `parquet`, `format`, `saveAsTable`\n",
    "- StructType (<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=structtype#pyspark.sql.types.StructType\" target=\"_blank\">Python</a>/<a href=\"http://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/types/StructType.html\" target=\"_blank\" target=\"_blank\">Scala</a>): `toDDL`\n",
    "\n",
    "##### Spark Types\n",
    "- Types (<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=types#module-pyspark.sql.types\" target=\"_blank\">Python</a>/<a href=\"http://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/types/index.html\" target=\"_blank\">Scala</a>): `ArrayType`, `DoubleType`, `IntegerType`, `LongType`, `StringType`, `StructType`, `StructField`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0984080c-5b18-4541-a8b8-2c9e42cf12ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read from CSV files\n",
    "Read from CSV with DataFrameReader's `csv` method and the following options:\n",
    "\n",
    "Tab separator, use first line as header, infer schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvPath = \"gs://info323-ya45-spring2023/notebooks/jupyter/2015-summary.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8dd8c7da-6a24-45d0-8d22-e394d711518c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .option(\"header\", True)\n",
    "  .option(\"inferSchema\", True)\n",
    "  .csv(csvPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e9f398a0-9656-4d92-b6e2-ce1676dc4604",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count|\n",
      "+-------------------------------------------+\n",
      "|                       United States,Rom...|\n",
      "|                       United States,Cro...|\n",
      "+-------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7e46eb91-57de-4ad6-aab5-b793a424d1a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPath = \"gs://info323-ya45-spring2023/notebooks/jupyter/users-500k/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "62ff5088-68f5-46fb-94b6-5820bc28cd1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "usersDF = (spark.read\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .option(\"header\", True)\n",
    "  .option(\"inferSchema\", True)\n",
    "  .csv(usersPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_first_touch_timestamp: long (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usersDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad7f567e-c0f9-46e2-a328-113747fd7257",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Manually define the schema by creating a `StructType` with column names and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3baf801d-4741-4610-97ce-0ba868f7af78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StringType, StructType, StructField\n",
    "\n",
    "userDefinedSchema = StructType([\n",
    "  StructField(\"user_id\", StringType(), True),  \n",
    "  StructField(\"user_first_touch_timestamp\", LongType(), True),\n",
    "  StructField(\"email\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "58437436-5f70-427b-81af-46994f78f16e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Read from CSV using this user-defined schema instead of inferring schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "49b4b64b-f40e-4d96-9930-7eabe5bdee9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usersDF = (spark.read\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .option(\"header\", True)\n",
    "  .schema(userDefinedSchema)\n",
    "  .csv(usersPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ccfd4a8a-ab53-41b3-8269-189f90997774",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Alternatively, define the schema using a DDL formatted string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a9d97b5d-c7ef-4bdf-ab52-4474ef9913dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DDLSchema = \"user_id string, user_first_touch_timestamp long, email string\"\n",
    "\n",
    "usersDF = (spark.read\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .option(\"header\", True)\n",
    "  .schema(DDLSchema)\n",
    "  .csv(usersPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath = \"gs://info323-ya45-spring2023/notebooks/jupyter/output/users-500k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "usersDF.write.option(\"header\", True).option(\"delimiter\", \"\\t\").csv(outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a3481b0a-ddba-4a56-ab5d-910d36819c77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read from JSON files\n",
    "\n",
    "Read from JSON with DataFrameReader's `json` method and the infer schema option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonPath = \"gs://info323-ya45-spring2023/notebooks/jupyter/2015-summary.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "af2b1bff-ec3a-4e4f-8a9b-173714ca1cf6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jdf = (spark.read\n",
    "  .option(\"inferSchema\", True)\n",
    "  .json(jsonPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "000f55de-7f0d-4c24-be1b-15e022a1f993",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jFolderPath = \"gs://info323-ya45-spring2023/notebooks/jupyter/events-500k/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4384fd87-139b-4a8c-a6fb-8e4fe11b1577",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "eventsDF = (spark.read\n",
    "  .option(\"inferSchema\", True)\n",
    "  .json(jFolderPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device: string (nullable = true)\n",
      " |-- ecommerce: struct (nullable = true)\n",
      " |    |-- purchase_revenue_in_usd: double (nullable = true)\n",
      " |    |-- total_item_quantity: long (nullable = true)\n",
      " |    |-- unique_items: long (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_previous_timestamp: long (nullable = true)\n",
      " |-- event_timestamp: long (nullable = true)\n",
      " |-- geo: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- coupon: string (nullable = true)\n",
      " |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |-- item_name: string (nullable = true)\n",
      " |    |    |-- item_revenue_in_usd: double (nullable = true)\n",
      " |    |    |-- price_in_usd: double (nullable = true)\n",
      " |    |    |-- quantity: long (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- user_first_touch_timestamp: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eventsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dccb93f2-918b-4453-8c9f-fd7720ac8949",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eventsDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jOutputPath = \"gs://info323-ya45-spring2023/notebooks/jupyter/output/events-500k/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3f752f16-001e-4114-88c8-895676cbeb15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "eventsDF.write.json(jOutputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "99c9b59f-ec38-45b5-9f2b-35e66d84a103",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Read data faster by creating a `StructType` with the schema names and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c92a6e25-43b7-460b-b4e1-2e7619ab9c37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, DoubleType, IntegerType, LongType, StringType, StructType, StructField\n",
    "\n",
    "userDefinedSchema = StructType([\n",
    "  StructField(\"device\", StringType(), True),  \n",
    "  StructField(\"ecommerce\", StructType([\n",
    "    StructField(\"purchaseRevenue\", DoubleType(), True),\n",
    "    StructField(\"total_item_quantity\", LongType(), True),\n",
    "    StructField(\"unique_items\", LongType(), True)\n",
    "  ]), True),\n",
    "  StructField(\"event_name\", StringType(), True),\n",
    "  StructField(\"event_previous_timestamp\", LongType(), True),\n",
    "  StructField(\"event_timestamp\", LongType(), True),\n",
    "  StructField(\"geo\", StructType([\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True)\n",
    "  ]), True),\n",
    "  StructField(\"items\", ArrayType(\n",
    "    StructType([\n",
    "      StructField(\"coupon\", StringType(), True),\n",
    "      StructField(\"item_id\", StringType(), True),\n",
    "      StructField(\"item_name\", StringType(), True),\n",
    "      StructField(\"item_revenue_in_usd\", DoubleType(), True),\n",
    "      StructField(\"price_in_usd\", DoubleType(), True),\n",
    "      StructField(\"quantity\", LongType(), True)\n",
    "    ])\n",
    "  ), True),\n",
    "  StructField(\"traffic_source\", StringType(), True),\n",
    "  StructField(\"user_first_touch_timestamp\", LongType(), True),\n",
    "  StructField(\"user_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "eventsDF = (spark.read\n",
    "  .schema(userDefinedSchema)\n",
    "  .json(jFolderPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d396d94e-0508-418b-8720-0faf12890cc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DDLSchema = \"`device` STRING,`ecommerce` STRUCT<`purchase_revenue_in_usd`: DOUBLE, `total_item_quantity`: BIGINT, `unique_items`: BIGINT>,`event_name` STRING,`event_previous_timestamp` BIGINT,`event_timestamp` BIGINT,`geo` STRUCT<`city`: STRING, `state`: STRING>,`items` ARRAY<STRUCT<`coupon`: STRING, `item_id`: STRING, `item_name`: STRING, `item_revenue_in_usd`: DOUBLE, `price_in_usd`: DOUBLE, `quantity`: BIGINT>>,`traffic_source` STRING,`user_first_touch_timestamp` BIGINT,`user_id` STRING\"\n",
    "\n",
    "eventsDF = (spark.read\n",
    "  .schema(DDLSchema)\n",
    "  .json(jFolderPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4fa591f2-e040-4a0b-84ef-69ecb592fcca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+------------------------+----------------+--------------------+-----+--------------+--------------------------+-----------------+\n",
      "| device|         ecommerce|event_name|event_previous_timestamp| event_timestamp|                 geo|items|traffic_source|user_first_touch_timestamp|          user_id|\n",
      "+-------+------------------+----------+------------------------+----------------+--------------------+-----+--------------+--------------------------+-----------------+\n",
      "|Android|{null, null, null}|mattresses|        1593445137069608|1593445139973236|      {Lakeland, FL}|   []|        google|          1593445100860131|UA000000106062296|\n",
      "|Android|{null, null, null}|  warranty|        1593448606629109|1593448809022141|     {Rock Hill, SC}|   []|     instagram|          1593448606629109|UA000000106082500|\n",
      "|Android|{null, null, null}|mattresses|        1593461473732425|1593461479624958|{Stone Mountain, GA}|   []|        google|          1593460845947854|UA000000106148969|\n",
      "+-------+------------------+----------+------------------------+----------------+--------------------+-----+--------------+--------------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "eventsDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a3a01e9f-ef8b-4fc3-bfae-5ef164010fc8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Write DataFrames to files\n",
    "\n",
    "Write `usersDF` to parquet with DataFrameWriter's `parquet` method and the following configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingDir = \"gs://info323-ya45-spring2023/notebooks/jupyter/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "29fa3c1b-de5f-4661-be72-3d6c8acc9607",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "usersOutputPath = workingDir + \"/users.parquet\"\n",
    "\n",
    "(usersDF.write\n",
    "  .option(\"compression\", \"snappy\")\n",
    "  .mode(\"overwrite\")\n",
    "  .parquet(usersOutputPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+------------------------+----------------+--------------------+-----+--------------+--------------------------+-----------------+\n",
      "| device|         ecommerce|event_name|event_previous_timestamp| event_timestamp|                 geo|items|traffic_source|user_first_touch_timestamp|          user_id|\n",
      "+-------+------------------+----------+------------------------+----------------+--------------------+-----+--------------+--------------------------+-----------------+\n",
      "|Android|{null, null, null}|mattresses|        1593445137069608|1593445139973236|      {Lakeland, FL}|   []|        google|          1593445100860131|UA000000106062296|\n",
      "|Android|{null, null, null}|  warranty|        1593448606629109|1593448809022141|     {Rock Hill, SC}|   []|     instagram|          1593448606629109|UA000000106082500|\n",
      "|Android|{null, null, null}|mattresses|        1593461473732425|1593461479624958|{Stone Mountain, GA}|   []|        google|          1593460845947854|UA000000106148969|\n",
      "+-------+------------------+----------+------------------------+----------------+--------------------+-----+--------------+--------------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "eventsDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f00ed8e-565e-4d25-9152-a1fbb77eed62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "eventsOutputPath = workingDir + \"/events.parquet\"\n",
    "\n",
    "(eventsDF.write\n",
    "  .option(\"compression\", \"snappy\")\n",
    "  .mode(\"overwrite\")\n",
    "  .parquet(eventsOutputPath)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}