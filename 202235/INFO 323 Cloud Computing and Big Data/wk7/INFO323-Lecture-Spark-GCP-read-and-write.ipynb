{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "53a2215b-9a8c-448f-a53a-f13c1b82dc5b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<h1 style=\"text-align:center\"> INFO 323: Cloud Computing and Big Data</h1>\n",
    "<h2 style=\"text-align:center\"> College of Computing and Informatics</h2>\n",
    "<h2 style=\"text-align:center\">Drexel University</h2>\n",
    "\n",
    "<h3 style=\"text-align:center\"> Spark Reader and Writer</h3>\n",
    "<h3 style=\"text-align:center\"> Yuan An, PhD</h3>\n",
    "<h3 style=\"text-align:center\">Associate Professor</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3473cdae-857b-4ae8-8699-e3c585cae8bd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Reader & Writer\n",
    "1. Read from CSV files\n",
    "1. Read from JSON files\n",
    "1. Write DataFrame to files\n",
    "1. Write DataFrame to tables\n",
    "\n",
    "##### Methods\n",
    "- DataFrameReader (<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dataframereader#pyspark.sql.DataFrameReader\" target=\"_blank\">Python</a>/<a href=\"http://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameReader.html\" target=\"_blank\">Scala</a>): `csv`, `json`, `option`, `schema`\n",
    "- DataFrameWriter (<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=dataframereader#pyspark.sql.DataFrameWriter\" target=\"_blank\">Python</a>/<a href=\"http://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameWriter.html\" target=\"_blank\">Scala</a>): `mode`, `option`, `parquet`, `format`, `saveAsTable`\n",
    "- StructType (<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=structtype#pyspark.sql.types.StructType\" target=\"_blank\">Python</a>/<a href=\"http://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/types/StructType.html\" target=\"_blank\" target=\"_blank\">Scala</a>): `toDDL`\n",
    "\n",
    "##### Spark Types\n",
    "- Types (<a href=\"https://spark.apache.org/docs/latest/api/python/pyspark.sql.html?highlight=types#module-pyspark.sql.types\" target=\"_blank\">Python</a>/<a href=\"http://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/types/index.html\" target=\"_blank\">Scala</a>): `ArrayType`, `DoubleType`, `IntegerType`, `LongType`, `StringType`, `StructType`, `StructField`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0984080c-5b18-4541-a8b8-2c9e42cf12ff",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read from CSV files\n",
    "Read from CSV with DataFrameReader's `csv` method and the following options:\n",
    "\n",
    "Tab separator, use first line as header, infer schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "csvPath = \"gs://info323-ya45-spring2023/notebooks/jupyter/2015-summary.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8dd8c7da-6a24-45d0-8d22-e394d711518c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = (spark.read\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .option(\"header\", True)\n",
    "  .option(\"inferSchema\", True)\n",
    "  .csv(csvPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e9f398a0-9656-4d92-b6e2-ce1676dc4604",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+\n",
      "|DEST_COUNTRY_NAME,ORIGIN_COUNTRY_NAME,count|\n",
      "+-------------------------------------------+\n",
      "|                       United States,Rom...|\n",
      "|                       United States,Cro...|\n",
      "+-------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7e46eb91-57de-4ad6-aab5-b793a424d1a6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "256"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "usersPath = \"gs://info323-ya45-spring2023/notebooks/jupyter/users-500k/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "62ff5088-68f5-46fb-94b6-5820bc28cd1e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "usersDF = (spark.read\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .option(\"header\", True)\n",
    "  .option(\"inferSchema\", True)\n",
    "  .csv(usersPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- user_first_touch_timestamp: long (nullable = true)\n",
      " |-- email: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "usersDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "usersDF.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad7f567e-c0f9-46e2-a328-113747fd7257",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Manually define the schema by creating a `StructType` with column names and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3baf801d-4741-4610-97ce-0ba868f7af78",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import LongType, StringType, StructType, StructField\n",
    "\n",
    "userDefinedSchema = StructType([\n",
    "  StructField(\"user_id\", StringType(), True),  \n",
    "  StructField(\"user_first_touch_timestamp\", LongType(), True),\n",
    "  StructField(\"email\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "58437436-5f70-427b-81af-46994f78f16e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Read from CSV using this user-defined schema instead of inferring schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "49b4b64b-f40e-4d96-9930-7eabe5bdee9c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "usersDF = (spark.read\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .option(\"header\", True)\n",
    "  .schema(userDefinedSchema)\n",
    "  .csv(usersPath))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ccfd4a8a-ab53-41b3-8269-189f90997774",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Alternatively, define the schema using a DDL formatted string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a9d97b5d-c7ef-4bdf-ab52-4474ef9913dd",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DDLSchema = \"user_id string, user_first_touch_timestamp long, email string\"\n",
    "\n",
    "usersDF = (spark.read\n",
    "  .option(\"sep\", \"\\t\")\n",
    "  .option(\"header\", True)\n",
    "  .schema(DDLSchema)\n",
    "  .csv(usersPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputPath = \"gs://info323-ya45-spring2023/notebooks/jupyter/output/users-500k\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "usersDF.write.option(\"header\", True).option(\"delimiter\", \"\\t\").csv(outputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a3481b0a-ddba-4a56-ab5d-910d36819c77",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Read from JSON files\n",
    "\n",
    "Read from JSON with DataFrameReader's `json` method and the infer schema option"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonPath = \"gs://info323-ya45-spring2023/notebooks/jupyter/2015-summary.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "af2b1bff-ec3a-4e4f-8a9b-173714ca1cf6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "jdf = (spark.read\n",
    "  .option(\"inferSchema\", True)\n",
    "  .json(jsonPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- DEST_COUNTRY_NAME: string (nullable = true)\n",
      " |-- ORIGIN_COUNTRY_NAME: string (nullable = true)\n",
      " |-- count: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jdf.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "000f55de-7f0d-4c24-be1b-15e022a1f993",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+-------------------+-----+\n",
      "|DEST_COUNTRY_NAME|ORIGIN_COUNTRY_NAME|count|\n",
      "+-----------------+-------------------+-----+\n",
      "|    United States|            Romania|   15|\n",
      "|    United States|            Croatia|    1|\n",
      "|    United States|            Ireland|  344|\n",
      "|            Egypt|      United States|   15|\n",
      "|    United States|              India|   62|\n",
      "|    United States|          Singapore|    1|\n",
      "|    United States|            Grenada|   62|\n",
      "|       Costa Rica|      United States|  588|\n",
      "|          Senegal|      United States|   40|\n",
      "|          Moldova|      United States|    1|\n",
      "+-----------------+-------------------+-----+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "jdf.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "jFolderPath = \"gs://info323-ya45-spring2023/notebooks/jupyter/events-500k/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4384fd87-139b-4a8c-a6fb-8e4fe11b1577",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "eventsDF = (spark.read\n",
    "  .option(\"inferSchema\", True)\n",
    "  .json(jFolderPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- device: string (nullable = true)\n",
      " |-- ecommerce: struct (nullable = true)\n",
      " |    |-- purchase_revenue_in_usd: double (nullable = true)\n",
      " |    |-- total_item_quantity: long (nullable = true)\n",
      " |    |-- unique_items: long (nullable = true)\n",
      " |-- event_name: string (nullable = true)\n",
      " |-- event_previous_timestamp: long (nullable = true)\n",
      " |-- event_timestamp: long (nullable = true)\n",
      " |-- geo: struct (nullable = true)\n",
      " |    |-- city: string (nullable = true)\n",
      " |    |-- state: string (nullable = true)\n",
      " |-- items: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- coupon: string (nullable = true)\n",
      " |    |    |-- item_id: string (nullable = true)\n",
      " |    |    |-- item_name: string (nullable = true)\n",
      " |    |    |-- item_revenue_in_usd: double (nullable = true)\n",
      " |    |    |-- price_in_usd: double (nullable = true)\n",
      " |    |    |-- quantity: long (nullable = true)\n",
      " |-- traffic_source: string (nullable = true)\n",
      " |-- user_first_touch_timestamp: long (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "eventsDF.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "dccb93f2-918b-4453-8c9f-fd7720ac8949",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "500000"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eventsDF.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "jOutputPath = \"gs://info323-ya45-spring2023/notebooks/jupyter/output/events-500k/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3f752f16-001e-4114-88c8-895676cbeb15",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "eventsDF.write.json(jOutputPath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "99c9b59f-ec38-45b5-9f2b-35e66d84a103",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Read data faster by creating a `StructType` with the schema names and data types"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c92a6e25-43b7-460b-b4e1-2e7619ab9c37",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import ArrayType, DoubleType, IntegerType, LongType, StringType, StructType, StructField\n",
    "\n",
    "userDefinedSchema = StructType([\n",
    "  StructField(\"device\", StringType(), True),  \n",
    "  StructField(\"ecommerce\", StructType([\n",
    "    StructField(\"purchaseRevenue\", DoubleType(), True),\n",
    "    StructField(\"total_item_quantity\", LongType(), True),\n",
    "    StructField(\"unique_items\", LongType(), True)\n",
    "  ]), True),\n",
    "  StructField(\"event_name\", StringType(), True),\n",
    "  StructField(\"event_previous_timestamp\", LongType(), True),\n",
    "  StructField(\"event_timestamp\", LongType(), True),\n",
    "  StructField(\"geo\", StructType([\n",
    "    StructField(\"city\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True)\n",
    "  ]), True),\n",
    "  StructField(\"items\", ArrayType(\n",
    "    StructType([\n",
    "      StructField(\"coupon\", StringType(), True),\n",
    "      StructField(\"item_id\", StringType(), True),\n",
    "      StructField(\"item_name\", StringType(), True),\n",
    "      StructField(\"item_revenue_in_usd\", DoubleType(), True),\n",
    "      StructField(\"price_in_usd\", DoubleType(), True),\n",
    "      StructField(\"quantity\", LongType(), True)\n",
    "    ])\n",
    "  ), True),\n",
    "  StructField(\"traffic_source\", StringType(), True),\n",
    "  StructField(\"user_first_touch_timestamp\", LongType(), True),\n",
    "  StructField(\"user_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "eventsDF = (spark.read\n",
    "  .schema(userDefinedSchema)\n",
    "  .json(jFolderPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d396d94e-0508-418b-8720-0faf12890cc4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DDLSchema = \"`device` STRING,`ecommerce` STRUCT<`purchase_revenue_in_usd`: DOUBLE, `total_item_quantity`: BIGINT, `unique_items`: BIGINT>,`event_name` STRING,`event_previous_timestamp` BIGINT,`event_timestamp` BIGINT,`geo` STRUCT<`city`: STRING, `state`: STRING>,`items` ARRAY<STRUCT<`coupon`: STRING, `item_id`: STRING, `item_name`: STRING, `item_revenue_in_usd`: DOUBLE, `price_in_usd`: DOUBLE, `quantity`: BIGINT>>,`traffic_source` STRING,`user_first_touch_timestamp` BIGINT,`user_id` STRING\"\n",
    "\n",
    "eventsDF = (spark.read\n",
    "  .schema(DDLSchema)\n",
    "  .json(jFolderPath))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4fa591f2-e040-4a0b-84ef-69ecb592fcca",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+------------------------+----------------+--------------------+-----+--------------+--------------------------+-----------------+\n",
      "| device|         ecommerce|event_name|event_previous_timestamp| event_timestamp|                 geo|items|traffic_source|user_first_touch_timestamp|          user_id|\n",
      "+-------+------------------+----------+------------------------+----------------+--------------------+-----+--------------+--------------------------+-----------------+\n",
      "|Android|{null, null, null}|mattresses|        1593445137069608|1593445139973236|      {Lakeland, FL}|   []|        google|          1593445100860131|UA000000106062296|\n",
      "|Android|{null, null, null}|  warranty|        1593448606629109|1593448809022141|     {Rock Hill, SC}|   []|     instagram|          1593448606629109|UA000000106082500|\n",
      "|Android|{null, null, null}|mattresses|        1593461473732425|1593461479624958|{Stone Mountain, GA}|   []|        google|          1593460845947854|UA000000106148969|\n",
      "+-------+------------------+----------+------------------------+----------------+--------------------+-----+--------------+--------------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "eventsDF.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a3a01e9f-ef8b-4fc3-bfae-5ef164010fc8",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Write DataFrames to files\n",
    "\n",
    "Write `usersDF` to parquet with DataFrameWriter's `parquet` method and the following configurations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "workingDir = \"gs://info323-ya45-spring2023/notebooks/jupyter/output\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "29fa3c1b-de5f-4661-be72-3d6c8acc9607",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "usersOutputPath = workingDir + \"/users.parquet\"\n",
    "\n",
    "(usersDF.write\n",
    "  .option(\"compression\", \"snappy\")\n",
    "  .mode(\"overwrite\")\n",
    "  .parquet(usersOutputPath)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+----------+------------------------+----------------+--------------------+-----+--------------+--------------------------+-----------------+\n",
      "| device|         ecommerce|event_name|event_previous_timestamp| event_timestamp|                 geo|items|traffic_source|user_first_touch_timestamp|          user_id|\n",
      "+-------+------------------+----------+------------------------+----------------+--------------------+-----+--------------+--------------------------+-----------------+\n",
      "|Android|{null, null, null}|mattresses|        1593445137069608|1593445139973236|      {Lakeland, FL}|   []|        google|          1593445100860131|UA000000106062296|\n",
      "|Android|{null, null, null}|  warranty|        1593448606629109|1593448809022141|     {Rock Hill, SC}|   []|     instagram|          1593448606629109|UA000000106082500|\n",
      "|Android|{null, null, null}|mattresses|        1593461473732425|1593461479624958|{Stone Mountain, GA}|   []|        google|          1593460845947854|UA000000106148969|\n",
      "+-------+------------------+----------+------------------------+----------------+--------------------+-----+--------------+--------------------------+-----------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "eventsDF.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f00ed8e-565e-4d25-9152-a1fbb77eed62",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "eventsOutputPath = workingDir + \"/events.parquet\"\n",
    "\n",
    "(eventsDF.write\n",
    "  .option(\"compression\", \"snappy\")\n",
    "  .mode(\"overwrite\")\n",
    "  .parquet(eventsOutputPath)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "INFO323-Lecture-presented-week6-Spark-5-read-and-write",
   "notebookOrigID": 364291157724015,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}