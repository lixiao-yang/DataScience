{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "aa4c0839-467c-44d0-a857-4eafba181e1b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "<h1 style=\"text-align:center\"> INFO 323: Cloud Computing and Big Data</h1>\n",
    "<h2 style=\"text-align:center\"> College of Computing and Informatics</h2>\n",
    "<h2 style=\"text-align:center\">Drexel University</h2>\n",
    "\n",
    "<h3 style=\"text-align:center\"> Structured API: Workding with Different Types of Data including Nulls</h3>\n",
    "<h3 style=\"text-align:center\"> Yuan An, PhD</h3>\n",
    "<h3 style=\"text-align:center\">Associate Professor</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ad594358-7158-4d69-999c-2d714d4ac0bc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Let’s read in the DataFrame that we’ll be using for this analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1bcba269-38a5-44a2-aefe-0fccb4886391",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ARR_AIRPORT_LAT: double (nullable = true)\n",
      " |-- ARR_AIRPORT_LON: double (nullable = true)\n",
      " |-- ARR_AIRPORT_TZOFFSET: double (nullable = true)\n",
      " |-- ARR_DELAY: double (nullable = true)\n",
      " |-- ARR_TIME: string (nullable = true)\n",
      " |-- CANCELLED: boolean (nullable = true)\n",
      " |-- CRS_ARR_TIME: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: string (nullable = true)\n",
      " |-- DEP_AIRPORT_LAT: double (nullable = true)\n",
      " |-- DEP_AIRPORT_LON: double (nullable = true)\n",
      " |-- DEP_AIRPORT_TZOFFSET: double (nullable = true)\n",
      " |-- DEP_DELAY: double (nullable = true)\n",
      " |-- DEP_TIME: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEST_AIRPORT_SEQ_ID: string (nullable = true)\n",
      " |-- DISTANCE: string (nullable = true)\n",
      " |-- DIVERTED: boolean (nullable = true)\n",
      " |-- FL_DATE: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT_SEQ_ID: string (nullable = true)\n",
      " |-- TAXI_IN: double (nullable = true)\n",
      " |-- TAXI_OUT: double (nullable = true)\n",
      " |-- UNIQUE_CARRIER: string (nullable = true)\n",
      " |-- WHEELS_OFF: string (nullable = true)\n",
      " |-- WHEELS_ON: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.read.format(\"json\")\\\n",
    "  .option(\"header\", \"true\")\\\n",
    "  .option(\"inferSchema\", \"true\")\\\n",
    "  .load(\"gs://info323-ya45-spring2023/flights/tzcorr/all_flights-00010-of-00026\")\n",
    "df.printSchema()\n",
    "df.createOrReplaceTempView(\"dfTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5dba9917-b020-4b98-920b-5a6f2a232f8f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+--------------------+---------+-------------------+---------+-------------------+-------------------+---------------+---------------+--------------------+---------+-------------------+----+-------------------+--------+--------+----------+------+---------------------+-------+--------+--------------+-------------------+-------------------+\n",
      "|ARR_AIRPORT_LAT|ARR_AIRPORT_LON|ARR_AIRPORT_TZOFFSET|ARR_DELAY|           ARR_TIME|CANCELLED|       CRS_ARR_TIME|       CRS_DEP_TIME|DEP_AIRPORT_LAT|DEP_AIRPORT_LON|DEP_AIRPORT_TZOFFSET|DEP_DELAY|           DEP_TIME|DEST|DEST_AIRPORT_SEQ_ID|DISTANCE|DIVERTED|   FL_DATE|ORIGIN|ORIGIN_AIRPORT_SEQ_ID|TAXI_IN|TAXI_OUT|UNIQUE_CARRIER|         WHEELS_OFF|          WHEELS_ON|\n",
      "+---------------+---------------+--------------------+---------+-------------------+---------+-------------------+-------------------+---------------+---------------+--------------------+---------+-------------------+----+-------------------+--------+--------+----------+------+---------------------+-------+--------+--------------+-------------------+-------------------+\n",
      "|    40.78833333|  -111.97777778|            -21600.0|    -10.0|2015-08-16T01:10:00|    false|2015-08-16T01:20:00|2015-08-15T23:32:00|    37.72138889|  -122.22111111|            -25200.0|     -3.0|2015-08-15T23:29:00| SLC|            1486903|  588.00|   false|2015-08-15|   OAK|              1379604|    8.0|    14.0|            OO|2015-08-15T23:43:00|2015-08-16T01:02:00|\n",
      "|    40.78833333|  -111.97777778|            -21600.0|     -2.0|2015-08-03T01:24:00|    false|2015-08-03T01:26:00|2015-08-02T23:40:00|    37.72138889|  -122.22111111|            -25200.0|     -1.0|2015-08-02T23:39:00| SLC|            1486903|  588.00|   false|2015-08-02|   OAK|              1379604|    3.0|    15.0|            DL|2015-08-02T23:54:00|2015-08-03T01:21:00|\n",
      "|    40.78833333|  -111.97777778|            -21600.0|     -5.0|2015-09-01T18:40:00|    false|2015-09-01T18:45:00|2015-09-01T17:05:00|    37.72138889|  -122.22111111|            -25200.0|     -2.0|2015-09-01T17:03:00| SLC|            1486903|  588.00|   false|2015-09-01|   OAK|              1379604|    6.0|     7.0|            WN|2015-09-01T17:10:00|2015-09-01T18:34:00|\n",
      "+---------------+---------------+--------------------+---------+-------------------+---------+-------------------+-------------------+---------------+---------------+--------------------+---------+-------------------+----+-------------------+--------+--------+----------+------+---------------------+-------+--------+--------------+-------------------+-------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "874b2805-2b34-4411-97b3-5ebe9847ce39",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Converting to Spark Types\n",
    "One thing you’ll see us do throughout this chapter is convert native types to Spark types. We do this\n",
    "by using the first function that we introduce here, the lit function. This function converts a type in\n",
    "another language to its correspnding Spark representation. Here’s how we can convert a couple of\n",
    "different kinds of Scala and Python values to their respective Spark types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e74cf95a-d935-4323-a786-e55fc02bad52",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----+---+\n",
      "|  5|five|5.0|\n",
      "+---+----+---+\n",
      "|  5|five|5.0|\n",
      "|  5|five|5.0|\n",
      "+---+----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import lit\n",
    "df.select(lit(5), lit(\"five\"), lit(5.0)).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1561f5f4-8bae-4cdb-ba8c-2650853d2765",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Working with Booleans\n",
    "Booleans are essential when it comes to data analysis because they are the foundation for all filtering.\n",
    "Boolean statements consist of four elements: and, or, true, and false. We use these simple structures\n",
    "to build logical statements that evaluate to either true or false. These statements are often used as\n",
    "conditional requirements for when a row of data must either pass the test (evaluate to true) or else it\n",
    "will be filtered out.\n",
    "Let’s use our retail dataset to explore working with Booleans. We can specify equality as well as\n",
    "less-than or greater-than:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b424801c-a6f0-4709-b384-a5f0230e33f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "206e834c-c1cc-4b5f-b47d-25e9271edd00",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "367"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.where(col(\"ORIGIN\") == 'PHL').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bab6b5b3-78f0-4eb1-b589-97704aad9bee",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------+--------------+\n",
      "|DEST|ORIGIN|UNIQUE_CARRIER|\n",
      "+----+------+--------------+\n",
      "|SLC |PHL   |US            |\n",
      "|SLC |PHL   |US            |\n",
      "|SLC |PHL   |DL            |\n",
      "|SLC |PHL   |DL            |\n",
      "|SLC |PHL   |AA            |\n",
      "|SLC |PHL   |AA            |\n",
      "|SLC |PHL   |AA            |\n",
      "+----+------+--------------+\n",
      "only showing top 7 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(col(\"ORIGIN\") == 'PHL').select(\"DEST\", \"ORIGIN\", \"UNIQUE_CARRIER\").show(7, False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "452937ec-4b95-4c4b-9891-5ca6401b5b3d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28233"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6a50808b-0846-4926-8b01-7043917c41ce",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Although you can specify your statements explicitly by using and if\n",
    "you like, they’re often easier to understand and to read if you specify them serially. or statements\n",
    "need to be specified in the same statement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "648c7ee2-b859-407a-bfa2-bb170070433a",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import instr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "42a8e3ee-a286-44b9-a621-426acc8c9085",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "depDelayFilter = col(\"DEP_DELAY\") > 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "366df2a5-cfa4-42fa-8e4a-b698bdb800b3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "destFilter = instr(df.DEST, \"SMF\") >= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "3f575772-6bb2-474e-99e5-1848ed32f5ab",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+---------------+--------------------+---------+-------------------+---------+-------------------+-------------------+---------------+---------------+--------------------+---------+-------------------+----+-------------------+--------+--------+----------+------+---------------------+-------+--------+--------------+-------------------+-------------------+\n",
      "|ARR_AIRPORT_LAT|ARR_AIRPORT_LON|ARR_AIRPORT_TZOFFSET|ARR_DELAY|           ARR_TIME|CANCELLED|       CRS_ARR_TIME|       CRS_DEP_TIME|DEP_AIRPORT_LAT|DEP_AIRPORT_LON|DEP_AIRPORT_TZOFFSET|DEP_DELAY|           DEP_TIME|DEST|DEST_AIRPORT_SEQ_ID|DISTANCE|DIVERTED|   FL_DATE|ORIGIN|ORIGIN_AIRPORT_SEQ_ID|TAXI_IN|TAXI_OUT|UNIQUE_CARRIER|         WHEELS_OFF|          WHEELS_ON|\n",
      "+---------------+---------------+--------------------+---------+-------------------+---------+-------------------+-------------------+---------------+---------------+--------------------+---------+-------------------+----+-------------------+--------+--------+----------+------+---------------------+-------+--------+--------------+-------------------+-------------------+\n",
      "|    40.78833333|  -111.97777778|            -25200.0|    237.0|2015-01-22T08:20:00|    false|2015-01-22T04:23:00|2015-01-21T23:10:00|    39.87222222|   -75.24083333|            -18000.0|    250.0|2015-01-22T03:20:00| SLC|            1486903| 1927.00|   false|2015-01-21|   PHL|              1410002|    9.0|    36.0|            US|2015-01-22T03:56:00|2015-01-22T08:11:00|\n",
      "|    40.78833333|  -111.97777778|            -21600.0|    315.0|2015-08-21T06:45:00|    false|2015-08-22T01:30:00|2015-08-20T20:33:00|    39.87222222|   -75.24083333|            -14400.0|    337.0|2015-08-21T02:10:00| SLC|            1486903| 1927.00|   false|2015-08-20|   PHL|              1410002|    4.0|    14.0|            DL|2015-08-21T02:24:00|2015-08-21T06:41:00|\n",
      "|    40.78833333|  -111.97777778|            -25200.0|    124.0|2015-03-01T06:27:00|    false|2015-03-01T04:23:00|2015-02-28T23:10:00|    39.87222222|   -75.24083333|            -18000.0|    117.0|2015-03-01T01:07:00| SLC|            1486903| 1927.00|   false|2015-02-28|   PHL|              1410002|    4.0|    30.0|            US|2015-03-01T01:37:00|2015-03-01T06:23:00|\n",
      "|    40.78833333|  -111.97777778|            -25200.0|    105.0|2015-02-23T06:08:00|    false|2015-02-23T04:23:00|2015-02-22T23:10:00|    39.87222222|   -75.24083333|            -18000.0|    104.0|2015-02-23T00:54:00| SLC|            1486903| 1927.00|   false|2015-02-22|   PHL|              1410002|   11.0|    13.0|            US|2015-02-23T01:07:00|2015-02-23T05:57:00|\n",
      "|    40.78833333|  -111.97777778|            -21600.0|    101.0|2015-10-01T02:46:00|    false|2015-10-01T01:05:00|2015-09-30T20:10:00|    39.87222222|   -75.24083333|            -14400.0|    108.0|2015-09-30T21:58:00| SLC|            1486903| 1927.00|   false|2015-09-30|   PHL|              1410002|    7.0|    21.0|            DL|2015-09-30T22:19:00|2015-10-01T02:39:00|\n",
      "|    40.78833333|  -111.97777778|            -21600.0|    152.0|2015-10-03T03:45:00|    false|2015-10-03T01:13:00|2015-10-02T20:06:00|    39.87222222|   -75.24083333|            -14400.0|    192.0|2015-10-02T23:18:00| SLC|            1486903| 1927.00|   false|2015-10-02|   PHL|              1410002|    4.0|    21.0|            DL|2015-10-02T23:39:00|2015-10-03T03:41:00|\n",
      "|    40.78833333|  -111.97777778|            -21600.0|     91.0|2015-08-25T04:59:00|    false|2015-08-25T03:28:00|2015-08-24T22:15:00|    39.87222222|   -75.24083333|            -14400.0|    113.0|2015-08-25T00:08:00| SLC|            1486903| 1927.00|   false|2015-08-24|   PHL|              1410002|    3.0|    24.0|            AA|2015-08-25T00:32:00|2015-08-25T04:56:00|\n",
      "|    40.78833333|  -111.97777778|            -25200.0|    118.0|2015-01-05T06:27:00|    false|2015-01-05T04:29:00|2015-01-04T23:10:00|    39.87222222|   -75.24083333|            -18000.0|    121.0|2015-01-05T01:11:00| SLC|            1486903| 1927.00|   false|2015-01-04|   PHL|              1410002|    5.0|    17.0|            US|2015-01-05T01:28:00|2015-01-05T06:22:00|\n",
      "|    40.78833333|  -111.97777778|            -21600.0|    266.0|2015-07-14T05:59:00|    false|2015-07-14T01:33:00|2015-07-13T20:37:00|    39.87222222|   -75.24083333|            -14400.0|    268.0|2015-07-14T01:05:00| SLC|            1486903| 1927.00|   false|2015-07-13|   PHL|              1410002|    7.0|    15.0|            DL|2015-07-14T01:20:00|2015-07-14T05:52:00|\n",
      "|    40.78833333|  -111.97777778|            -25200.0|    136.0|2015-03-06T06:39:00|    false|2015-03-06T04:23:00|2015-03-05T23:10:00|    39.87222222|   -75.24083333|            -18000.0|    150.0|2015-03-06T01:40:00| SLC|            1486903| 1927.00|   false|2015-03-05|   PHL|              1410002|   10.0|    33.0|            US|2015-03-06T02:13:00|2015-03-06T06:29:00|\n",
      "|    38.69555556|  -121.59083333|            -25200.0|    -19.0|2015-07-03T03:24:00|    false|2015-07-03T03:43:00|2015-07-02T21:45:00|    39.87222222|   -75.24083333|            -14400.0|     -4.0|2015-07-02T21:41:00| SMF|            1489302| 2458.00|   false|2015-07-02|   PHL|              1410002|    6.0|    16.0|            AA|2015-07-02T21:57:00|2015-07-03T03:18:00|\n",
      "|    38.69555556|  -121.59083333|            -25200.0|    -28.0|2015-07-07T03:15:00|    false|2015-07-07T03:43:00|2015-07-06T21:45:00|    39.87222222|   -75.24083333|            -14400.0|     -4.0|2015-07-06T21:41:00| SMF|            1489302| 2458.00|   false|2015-07-06|   PHL|              1410002|    5.0|    10.0|            AA|2015-07-06T21:51:00|2015-07-07T03:10:00|\n",
      "|    38.69555556|  -121.59083333|            -25200.0|    -11.0|2015-08-03T03:32:00|    false|2015-08-03T03:43:00|2015-08-02T21:45:00|    39.87222222|   -75.24083333|            -14400.0|     13.0|2015-08-02T21:58:00| SMF|            1489302| 2458.00|   false|2015-08-02|   PHL|              1410002|    5.0|    19.0|            AA|2015-08-02T22:17:00|2015-08-03T03:27:00|\n",
      "|    38.69555556|  -121.59083333|            -25200.0|    -12.0|2015-07-13T03:31:00|    false|2015-07-13T03:43:00|2015-07-12T21:45:00|    39.87222222|   -75.24083333|            -14400.0|     -2.0|2015-07-12T21:43:00| SMF|            1489302| 2458.00|   false|2015-07-12|   PHL|              1410002|    7.0|    18.0|            AA|2015-07-12T22:01:00|2015-07-13T03:24:00|\n",
      "|    38.69555556|  -121.59083333|            -25200.0|     -8.0|2015-07-28T03:35:00|    false|2015-07-28T03:43:00|2015-07-27T21:45:00|    39.87222222|   -75.24083333|            -14400.0|      3.0|2015-07-27T21:48:00| SMF|            1489302| 2458.00|   false|2015-07-27|   PHL|              1410002|    7.0|    18.0|            AA|2015-07-27T22:06:00|2015-07-28T03:28:00|\n",
      "|    38.69555556|  -121.59083333|            -25200.0|    -12.0|2015-08-07T03:31:00|    false|2015-08-07T03:43:00|2015-08-06T21:45:00|    39.87222222|   -75.24083333|            -14400.0|     13.0|2015-08-06T21:58:00| SMF|            1489302| 2458.00|   false|2015-08-06|   PHL|              1410002|    6.0|    15.0|            AA|2015-08-06T22:13:00|2015-08-07T03:25:00|\n",
      "|    38.69555556|  -121.59083333|            -25200.0|    -13.0|2015-06-16T03:30:00|    false|2015-06-16T03:43:00|2015-06-15T21:45:00|    39.87222222|   -75.24083333|            -14400.0|      7.0|2015-06-15T21:52:00| SMF|            1489302| 2458.00|   false|2015-06-15|   PHL|              1410002|    4.0|    15.0|            US|2015-06-15T22:07:00|2015-06-16T03:26:00|\n",
      "|    38.69555556|  -121.59083333|            -25200.0|     -9.0|2015-06-28T03:34:00|    false|2015-06-28T03:43:00|2015-06-27T21:45:00|    39.87222222|   -75.24083333|            -14400.0|      6.0|2015-06-27T21:51:00| SMF|            1489302| 2458.00|   false|2015-06-27|   PHL|              1410002|    5.0|    28.0|            US|2015-06-27T22:19:00|2015-06-28T03:29:00|\n",
      "|    38.69555556|  -121.59083333|            -25200.0|    -10.0|2015-08-17T03:33:00|    false|2015-08-17T03:43:00|2015-08-16T21:45:00|    39.87222222|   -75.24083333|            -14400.0|     10.0|2015-08-16T21:55:00| SMF|            1489302| 2458.00|   false|2015-08-16|   PHL|              1410002|    7.0|    17.0|            AA|2015-08-16T22:12:00|2015-08-17T03:26:00|\n",
      "|    38.69555556|  -121.59083333|            -25200.0|     -7.0|2015-06-08T03:36:00|    false|2015-06-08T03:43:00|2015-06-07T21:45:00|    39.87222222|   -75.24083333|            -14400.0|     -3.0|2015-06-07T21:42:00| SMF|            1489302| 2458.00|   false|2015-06-07|   PHL|              1410002|    6.0|    11.0|            US|2015-06-07T21:53:00|2015-06-08T03:30:00|\n",
      "+---------------+---------------+--------------------+---------+-------------------+---------+-------------------+-------------------+---------------+---------------+--------------------+---------+-------------------+----+-------------------+--------+--------+----------+------+---------------------+-------+--------+--------------+-------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.where(df.ORIGIN.isin(\"PHL\")).where(destFilter | depDelayFilter).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "914ed7f1-4953-4ba1-8dd5-f992297ec65f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Boolean expressions are not just reserved to filters. To filter a DataFrame, you can also just specify a\n",
    "Boolean column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0e95feaa-894d-474d-95b2-59e30ccf8706",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "originFilter = col(\"ORIGIN\") == \"PHL\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "4c5ec547-d59b-47a4-91f2-5947444e8ad6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|ARR_DELAY|\n",
      "+---------+\n",
      "|    237.0|\n",
      "|    315.0|\n",
      "|    124.0|\n",
      "|    105.0|\n",
      "|    101.0|\n",
      "|    152.0|\n",
      "|     91.0|\n",
      "|    118.0|\n",
      "|    266.0|\n",
      "|    136.0|\n",
      "|    -19.0|\n",
      "|    -28.0|\n",
      "|    -11.0|\n",
      "|    -12.0|\n",
      "|     -8.0|\n",
      "|    -12.0|\n",
      "|    -13.0|\n",
      "|     -9.0|\n",
      "|    -10.0|\n",
      "|     -7.0|\n",
      "+---------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.withColumn(\"FromPhil\", originFilter & (depDelayFilter | destFilter)).where(\"FromPhil\").select(\"ARR_DELAY\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a3b8d5d6-c52f-48f9-affe-c022061868f0",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If you’re coming from a SQL background, all of these statements should seem quite familiar. Indeed,\n",
    "all of them can be expressed as a where clause. In fact, it’s often easier to just express filters as SQL\n",
    "statements than using the programmatic DataFrame interface and Spark SQL allows us to do this\n",
    "without paying any performance penalty."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "31f72af5-7420-4101-81f2-a1b43fcf3158",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Working with Numbers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1a69893f-8bf8-43de-ab4b-a00b058bcf2a",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "When working with big data, the second most common task you will do after filtering things is\n",
    "counting things. For the most part, we simply need to express our computation, and that should be\n",
    "valid assuming that we’re working with numerical data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ARR_AIRPORT_LAT: double (nullable = true)\n",
      " |-- ARR_AIRPORT_LON: double (nullable = true)\n",
      " |-- ARR_AIRPORT_TZOFFSET: double (nullable = true)\n",
      " |-- ARR_DELAY: double (nullable = true)\n",
      " |-- ARR_TIME: string (nullable = true)\n",
      " |-- CANCELLED: boolean (nullable = true)\n",
      " |-- CRS_ARR_TIME: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: string (nullable = true)\n",
      " |-- DEP_AIRPORT_LAT: double (nullable = true)\n",
      " |-- DEP_AIRPORT_LON: double (nullable = true)\n",
      " |-- DEP_AIRPORT_TZOFFSET: double (nullable = true)\n",
      " |-- DEP_DELAY: double (nullable = true)\n",
      " |-- DEP_TIME: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEST_AIRPORT_SEQ_ID: string (nullable = true)\n",
      " |-- DISTANCE: string (nullable = true)\n",
      " |-- DIVERTED: boolean (nullable = true)\n",
      " |-- FL_DATE: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT_SEQ_ID: string (nullable = true)\n",
      " |-- TAXI_IN: double (nullable = true)\n",
      " |-- TAXI_OUT: double (nullable = true)\n",
      " |-- UNIQUE_CARRIER: string (nullable = true)\n",
      " |-- WHEELS_OFF: string (nullable = true)\n",
      " |-- WHEELS_ON: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6214a2a8-5c1c-42d6-b3ed-9ae5174954ff",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|UNIQUE_CARRIER|realDelay|\n",
      "+--------------+---------+\n",
      "|            OO|    905.0|\n",
      "|            DL|      9.0|\n",
      "+--------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import expr, pow\n",
    "fabricatedQuantity = pow(col(\"DEP_DELAY\") * col(\"ARR_DELAY\"), 2) + 5\n",
    "df.select(expr(\"UNIQUE_CARRIER\"), fabricatedQuantity.alias(\"realDelay\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d15eedc0-cd8a-42c0-8231-193eb3916833",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Notice that we were able to multiply our columns together because they were both numerical.\n",
    "Naturally we can add and subtract as necessary, as well. In fact, we can do all of this as a SQL\n",
    "expression, as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f214798-b9c9-4187-940e-4546212e7048",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+---------+\n",
      "|UNIQUE_CARRIER|realDelay|\n",
      "+--------------+---------+\n",
      "|            OO|    905.0|\n",
      "|            DL|      9.0|\n",
      "+--------------+---------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.selectExpr(\n",
    "  \"UNIQUE_CARRIER\",\n",
    "  \"(POWER((DEP_DELAY * ARR_DELAY), 2.0) + 5) as realDelay\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "98243c54-3bf4-46ff-a836-2246b4b9f977",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Another common numerical task is rounding. If you’d like to just round to a whole number, oftentimes\n",
    "you can cast the value to an integer and that will work just fine. However, Spark also has more\n",
    "detailed functions for performing this explicitly and to a certain level of precision. In the following\n",
    "example, we round to one decimal place:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "af2d45ba-8b72-4318-9900-c35562888469",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+--------------+\n",
      "|round(2.5, 0)|bround(2.5, 0)|\n",
      "+-------------+--------------+\n",
      "|          3.0|           2.0|\n",
      "|          3.0|           2.0|\n",
      "+-------------+--------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import lit, round, bround\n",
    "\n",
    "df.select(round(lit(\"2.5\")), bround(lit(\"2.5\"))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6bc3f9a8-0bfb-4e9e-916a-ad14e027f020",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Another numerical task is to compute the correlation of two columns. For example, we can see the\n",
    "Pearson correlation coefficient for two columns to see if cheaper things are typically bought in\n",
    "greater quantities. We can do this through a function as well as through the DataFrame statistic\n",
    "methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "e8ea5508-c5d2-47d6-a051-958ef7e68256",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0d93832e-9cf9-4322-b851-d6e954dcc52f",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9476274949356805"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.stat.corr(\"DEP_DELAY\", \"ARR_DELAY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "99c03f2c-306b-4775-96fe-fbbdacb48fb4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9523863105799023"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.select(corr(\"DEP_DELAY\", \"ARR_DELAY\")).take(1)[0][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "44408a3c-e4c8-476f-b716-40b679124a70",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Another common task is to compute summary statistics for a column or set of columns. We can use the\n",
    "describe method to achieve exactly this. This will take all numeric columns and calculate the count,\n",
    "mean, standard deviation, min, and max. You should use this primarily for viewing in the console\n",
    "because the schema might change in the future:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6acae363-e416-4328-adbf-ed13bfdf4005",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ARR_AIRPORT_LAT</th>\n",
       "      <th>ARR_AIRPORT_LON</th>\n",
       "      <th>ARR_AIRPORT_TZOFFSET</th>\n",
       "      <th>ARR_DELAY</th>\n",
       "      <th>DEP_AIRPORT_LAT</th>\n",
       "      <th>DEP_AIRPORT_LON</th>\n",
       "      <th>DEP_AIRPORT_TZOFFSET</th>\n",
       "      <th>DEP_DELAY</th>\n",
       "      <th>TAXI_IN</th>\n",
       "      <th>TAXI_OUT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>28233.000000</td>\n",
       "      <td>28233.000000</td>\n",
       "      <td>28233.000000</td>\n",
       "      <td>28012.000000</td>\n",
       "      <td>28233.000000</td>\n",
       "      <td>28233.000000</td>\n",
       "      <td>28233.000000</td>\n",
       "      <td>28068.000000</td>\n",
       "      <td>28051.000000</td>\n",
       "      <td>28057.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>39.753249</td>\n",
       "      <td>-116.732381</td>\n",
       "      <td>-24408.798215</td>\n",
       "      <td>3.675960</td>\n",
       "      <td>37.102959</td>\n",
       "      <td>-112.732000</td>\n",
       "      <td>-24565.380937</td>\n",
       "      <td>8.379364</td>\n",
       "      <td>5.382018</td>\n",
       "      <td>15.272980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.046346</td>\n",
       "      <td>4.806332</td>\n",
       "      <td>3128.808671</td>\n",
       "      <td>31.943579</td>\n",
       "      <td>5.131649</td>\n",
       "      <td>13.146600</td>\n",
       "      <td>4152.777185</td>\n",
       "      <td>30.839050</td>\n",
       "      <td>2.649147</td>\n",
       "      <td>7.721619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>38.695556</td>\n",
       "      <td>-121.590833</td>\n",
       "      <td>-28800.000000</td>\n",
       "      <td>-56.000000</td>\n",
       "      <td>20.898611</td>\n",
       "      <td>-157.920278</td>\n",
       "      <td>-36000.000000</td>\n",
       "      <td>-24.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>38.695556</td>\n",
       "      <td>-121.590833</td>\n",
       "      <td>-25200.000000</td>\n",
       "      <td>-11.000000</td>\n",
       "      <td>33.434167</td>\n",
       "      <td>-121.929167</td>\n",
       "      <td>-25200.000000</td>\n",
       "      <td>-5.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>40.788333</td>\n",
       "      <td>-111.977778</td>\n",
       "      <td>-25200.000000</td>\n",
       "      <td>-4.000000</td>\n",
       "      <td>34.200556</td>\n",
       "      <td>-117.601111</td>\n",
       "      <td>-25200.000000</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>14.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>40.788333</td>\n",
       "      <td>-111.977778</td>\n",
       "      <td>-21600.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>39.861667</td>\n",
       "      <td>-112.011667</td>\n",
       "      <td>-25200.000000</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>18.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>40.788333</td>\n",
       "      <td>-111.977778</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>674.000000</td>\n",
       "      <td>47.450000</td>\n",
       "      <td>-71.006389</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>694.000000</td>\n",
       "      <td>55.000000</td>\n",
       "      <td>151.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       ARR_AIRPORT_LAT  ARR_AIRPORT_LON  ARR_AIRPORT_TZOFFSET     ARR_DELAY  \\\n",
       "count     28233.000000     28233.000000          28233.000000  28012.000000   \n",
       "mean         39.753249      -116.732381         -24408.798215      3.675960   \n",
       "std           1.046346         4.806332           3128.808671     31.943579   \n",
       "min          38.695556      -121.590833         -28800.000000    -56.000000   \n",
       "25%          38.695556      -121.590833         -25200.000000    -11.000000   \n",
       "50%          40.788333      -111.977778         -25200.000000     -4.000000   \n",
       "75%          40.788333      -111.977778         -21600.000000      6.000000   \n",
       "max          40.788333      -111.977778              0.000000    674.000000   \n",
       "\n",
       "       DEP_AIRPORT_LAT  DEP_AIRPORT_LON  DEP_AIRPORT_TZOFFSET     DEP_DELAY  \\\n",
       "count     28233.000000     28233.000000          28233.000000  28068.000000   \n",
       "mean         37.102959      -112.732000         -24565.380937      8.379364   \n",
       "std           5.131649        13.146600           4152.777185     30.839050   \n",
       "min          20.898611      -157.920278         -36000.000000    -24.000000   \n",
       "25%          33.434167      -121.929167         -25200.000000     -5.000000   \n",
       "50%          34.200556      -117.601111         -25200.000000     -1.000000   \n",
       "75%          39.861667      -112.011667         -25200.000000      8.000000   \n",
       "max          47.450000       -71.006389              0.000000    694.000000   \n",
       "\n",
       "            TAXI_IN      TAXI_OUT  \n",
       "count  28051.000000  28057.000000  \n",
       "mean       5.382018     15.272980  \n",
       "std        2.649147      7.721619  \n",
       "min        1.000000      1.000000  \n",
       "25%        4.000000     10.000000  \n",
       "50%        5.000000     14.000000  \n",
       "75%        6.000000     18.000000  \n",
       "max       55.000000    151.000000  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.toPandas().describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5f021963-2a03-4e34-9fdf-77bc30e8c63b",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+------------------+------------------+\n",
      "|DEP_DELAY        |DISTANCE          |ARR_DELAY         |\n",
      "+-----------------+------------------+------------------+\n",
      "|28068            |28233             |28012             |\n",
      "|8.379364400741057|749.5296992880671 |3.6759603027274026|\n",
      "|30.8390501393679 |481.00236948166906|31.943579377647556|\n",
      "|-24.0            |1087.00           |-56.0             |\n",
      "|694.0            |909.00            |674.0             |\n",
      "+-----------------+------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.describe().select(\"DEP_DELAY\", \"DISTANCE\", \"ARR_DELAY\").show(10, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6d8f0326-aac3-420c-90e7-be0e0a5ad7fa",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "If you need these exact numbers, you can also perform this as an aggregation yourself by importing the\n",
    "functions and applying them to the columns that you need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1f580dfc-d547-4fdf-bd47-8aa69eee199d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import count, mean, stddev_pop, min, max"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "15f8f69d-798f-4da6-b94c-c3c8e1e3165e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "There are a number of statistical functions available in the StatFunctions Package (accessible using\n",
    "stat as we see in the code block below). These are DataFrame methods that you can use to calculate\n",
    "a variety of different things. For instance, you can calculate either exact or approximate quantiles of\n",
    "your data using the approxQuantile method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "61003965-0c40-44ca-9256-4d1d7f571594",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[-4.0]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "colName = \"ARR_DELAY\"\n",
    "quantileProbs = [0.5]\n",
    "relError = 0.05\n",
    "df.stat.approxQuantile(\"ARR_DELAY\", quantileProbs, relError) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fe48646b-f373-4503-97e1-7cc05917b776",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------------+\n",
      "|   avg(DEP_DELAY)|\n",
      "+-----------------+\n",
      "|8.379364400741057|\n",
      "+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.select(mean(\"DEP_DELAY\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cb94f037-2266-4d48-9184-5ee13ccade87",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "You also can use this to see a cross-tabulation or frequent item pairs (be careful, this output will be\n",
    "large:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "6f1f40f8-444e-4bb2-b027-bc374dc81573",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+---+---+----+---+---+----+---+---+---+---+---+----+----+---+---+---+---+---+----+----+----+---+----+----+---+----+----+---+---+---+---+\n",
      "|DEST_ORIGIN|ATL|BOS| BUR|CLT|DAL| DEN|DFW|HNL|IAD|IAH|JFK| LAS| LAX|LGB|MDW|MSP|OAK|OGG| ONT| ORD| PDX|PHL| PHX| SAN|SAT| SEA| SFO|SJC|SMF|SNA|STL|\n",
      "+-----------+---+---+----+---+---+----+---+---+---+---+---+----+----+---+---+---+---+---+----+----+----+---+----+----+---+----+----+---+---+---+---+\n",
      "|        SMF|286| 22|1136|113|141|1307|679|173|143|352|156|1382|3039|355|236|286|  0|188|1127| 279| 900| 37|1627|   0|  0|   0|   0|  0|  0|  0|  0|\n",
      "|        SLC|  0|  0|   0|  0|  0|   0|  0|  0|  0|  0|  0|   0|   0|  0|  0|  0|218|  0|   0|1457|1268|330|2958|1201|291|1953|1800|916|791|822|264|\n",
      "+-----------+---+---+----+---+---+----+---+---+---+---+---+----+----+---+---+---+---+---+----+----+----+---+----+----+---+----+----+---+---+---+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.stat.crosstab(\"DEST\", \"ORIGIN\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|DEST_freqItems|ORIGIN_freqItems                                                                                                                                           |\n",
      "+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|[SMF, SLC]    |[BUR, LGB, PHL, JFK, OAK, SMF, DFW, CLT, SJC, ORD, IAH, OGG, SNA, DEN, ONT, PDX, HNL, STL, IAD, MSP, LAS, DAL, ATL, SFO, SEA, SAT, MDW, PHX, SAN, BOS, LAX]|\n",
      "+--------------+-----------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.stat.freqItems([\"DEST\", \"ORIGIN\"]).show(2, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ec17ba86-76f5-444d-bc68-fb209152e286",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Working with Strings\n",
    "String manipulation shows up in nearly every data flow, and it’s worth explaining what you can do\n",
    "with strings. You might be manipulating log files performing regular expression extraction or\n",
    "substitution, or checking for simple string existence, or making all strings uppercase or lowercase.\n",
    "\n",
    "Let’s begin with the last task because it’s the most straightforward. The initcap function will\n",
    "capitalize every word in a given string when that word is separated from another by a space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ARR_AIRPORT_LAT: double (nullable = true)\n",
      " |-- ARR_AIRPORT_LON: double (nullable = true)\n",
      " |-- ARR_AIRPORT_TZOFFSET: double (nullable = true)\n",
      " |-- ARR_DELAY: double (nullable = true)\n",
      " |-- ARR_TIME: string (nullable = true)\n",
      " |-- CANCELLED: boolean (nullable = true)\n",
      " |-- CRS_ARR_TIME: string (nullable = true)\n",
      " |-- CRS_DEP_TIME: string (nullable = true)\n",
      " |-- DEP_AIRPORT_LAT: double (nullable = true)\n",
      " |-- DEP_AIRPORT_LON: double (nullable = true)\n",
      " |-- DEP_AIRPORT_TZOFFSET: double (nullable = true)\n",
      " |-- DEP_DELAY: double (nullable = true)\n",
      " |-- DEP_TIME: string (nullable = true)\n",
      " |-- DEST: string (nullable = true)\n",
      " |-- DEST_AIRPORT_SEQ_ID: string (nullable = true)\n",
      " |-- DISTANCE: string (nullable = true)\n",
      " |-- DIVERTED: boolean (nullable = true)\n",
      " |-- FL_DATE: string (nullable = true)\n",
      " |-- ORIGIN: string (nullable = true)\n",
      " |-- ORIGIN_AIRPORT_SEQ_ID: string (nullable = true)\n",
      " |-- TAXI_IN: double (nullable = true)\n",
      " |-- TAXI_OUT: double (nullable = true)\n",
      " |-- UNIQUE_CARRIER: string (nullable = true)\n",
      " |-- WHEELS_OFF: string (nullable = true)\n",
      " |-- WHEELS_ON: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c8b1e129-1d06-4ebe-91a5-3cc09818a290",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------+\n",
      "|initcap(DEST)|\n",
      "+-------------+\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "|          Slc|\n",
      "+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import initcap, col\n",
    "df.select(initcap(col(\"DEST\"))).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "b9230a67-e85a-4eff-91b0-29d780033c1c",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "As just mentioned, you can cast strings in uppercase and lowercase, as well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "326654d6-b572-4c88-9a85-4dc2533cf520",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-------------+--------------------+\n",
      "|ORIGIN|lower(ORIGIN)|upper(lower(ORIGIN))|\n",
      "+------+-------------+--------------------+\n",
      "|   OAK|          oak|                 OAK|\n",
      "|   OAK|          oak|                 OAK|\n",
      "+------+-------------+--------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import lower, upper\n",
    "df.select(col(\"ORIGIN\"),\n",
    "    lower(col(\"ORIGIN\")),\n",
    "    upper(lower(col(\"ORIGIN\")))).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "2824eea1-3c1b-42bf-b5bc-222fa999f9a1",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Another trivial task is adding or removing spaces around a string. You can do this by using lpad,\n",
    "ltrim, rpad and rtrim, trim:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a75a0733-70ad-4b05-81fe-11c4c73bd761",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+---------+-----+---+----------+\n",
      "|    ltrim|    rtrim| trim| lp|        rp|\n",
      "+---------+---------+-----+---+----------+\n",
      "|HELLO    |    HELLO|HELLO|HEL|HELLO     |\n",
      "|HELLO    |    HELLO|HELLO|HEL|HELLO     |\n",
      "+---------+---------+-----+---+----------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import lit, ltrim, rtrim, rpad, lpad, trim\n",
    "df.select(\n",
    "    ltrim(lit(\"    HELLO    \")).alias(\"ltrim\"),\n",
    "    rtrim(lit(\"    HELLO    \")).alias(\"rtrim\"),\n",
    "    trim(lit(\"    HELLO    \")).alias(\"trim\"),\n",
    "    lpad(lit(\"HELLO\"), 3, \" \").alias(\"lp\"),\n",
    "    rpad(lit(\"HELLO\"), 10, \" \").alias(\"rp\")).show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8190768e-854d-4fa1-b76a-2e6d96a7b1ec",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Note that if lpad or rpad takes a number less than the length of the string, it will always remove\n",
    "values from the right side of the string.\n",
    "\n",
    "## Regular Expressions\n",
    "Probably one of the most frequently performed tasks is searching for the existence of one string in\n",
    "another or replacing all mentions of a string with another value. \n",
    "\n",
    "Spark takes advantage of the complete power of Java regular expressions. There are two key functions in Spark that you’ll need in order to\n",
    "perform regular expression tasks: regexp_extract and regexp_replace. These functions extract\n",
    "values and replace values, respectively.\n",
    "\n",
    "Let’s explore how to use the regexp_replace function to replace substitute color names in our\n",
    "description column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ef667248-bf45-49c7-bf42-600793e1a82e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+\n",
      "|Origin|DEST|\n",
      "+------+----+\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "|  PHIL| SLC|\n",
      "+------+----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "regex_string = \"PHL|LAX|OAK\"\n",
    "df.select(\n",
    "  regexp_replace(col(\"ORIGIN\"), regex_string, \"PHIL\").alias(\"Origin\"),\n",
    "  col(\"DEST\")).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "915d44d5-fb3c-4b96-ab6c-dcf4d645aee9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Another task might be to replace given characters with other characters. Building this as a regular\n",
    "expression could be tedious, so Spark also provides the translate function to replace these values.\n",
    "This is done at the character level and will replace all instances of a character with the indexed\n",
    "character in the replacement string:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bce55706-17f5-47c6-883c-44baa6862514",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Sometimes, rather than extracting values, we simply want to check for their existence. We can do this\n",
    "with the contains method on each column. This will return a Boolean declaring whether the value\n",
    "you specify is in the column’s string. In Python and SQL, we can use the instr function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "92da60ab-9335-4cc5-85e8-194fb40a786c",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|ORIGIN|\n",
      "+------+\n",
      "|PHL   |\n",
      "|PHL   |\n",
      "|PHL   |\n",
      "+------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import instr\n",
    "containsPH = instr(col(\"ORIGIN\"), \"PH\") >= 1\n",
    "containsLA = instr(col(\"ORIGIN\"), \"LA\") >= 1\n",
    "df.withColumn(\"fromPHLA\", containsPH | containsLA)\\\n",
    "  .where(\"fromPHLA\")\\\n",
    "  .select(\"ORIGIN\").show(3, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8ae0a358-7f76-48cf-b9c8-e05add82e2cc",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## Working with Dates and Timestamps\n",
    "\n",
    "Spark will make a best effort to correctly identify\n",
    "column types, including dates and timestamps when we enable inferSchema. \n",
    "\n",
    "Although Spark will do read dates or times on a best-effort basis. However, sometimes there will be\n",
    "no getting around working with strangely formatted dates and times. The key to understanding the\n",
    "transformations that you are going to need to apply is to ensure that you know exactly what type and\n",
    "format you have at each given step of the way. Another common “gotcha” is that Spark’s\n",
    "TimestampType class supports only second-level precision, which means that if you’re going to be\n",
    "working with milliseconds or microseconds, you’ll need to work around this problem by potentially\n",
    "operating on them as longs. Any more precision when coercing to a TimestampType will be\n",
    "removed.\n",
    "\n",
    "Spark can be a bit particular about what format you have at any given point in time. It’s important to\n",
    "be explicit when parsing or converting to ensure that there are no issues in doing so. At the end of the\n",
    "day, Spark is working with Java dates and timestamps and therefore conforms to those standards.\n",
    "\n",
    "Let’s begin with the basics and get the current date and the current timestamps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "cf94e6dc-79e0-43b0-98b8-31edf29c9837",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import current_date, current_timestamp\n",
    "dateDF = spark.range(10)\\\n",
    "  .withColumn(\"today\", current_date())\\\n",
    "  .withColumn(\"now\", current_timestamp())\n",
    "dateDF.createOrReplaceTempView(\"dateTable\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d797737d-95b2-4c9d-9d61-bc11f2475f96",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+----------+--------------------+\n",
      "| id|     today|                 now|\n",
      "+---+----------+--------------------+\n",
      "|  0|2023-05-14|2023-05-14 19:43:...|\n",
      "|  1|2023-05-14|2023-05-14 19:43:...|\n",
      "|  2|2023-05-14|2023-05-14 19:43:...|\n",
      "|  3|2023-05-14|2023-05-14 19:43:...|\n",
      "+---+----------+--------------------+\n",
      "only showing top 4 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.show(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "831a2ae4-5106-48fb-8948-78f1e94dc04e",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Now that we have a simple DataFrame to work with, let’s add and subtract five days from today.\n",
    "These functions take a column and then the number of days to either add or subtract as the arguments:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "bf249e5c-63d4-4889-a6ae-421376656a40",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------+------------------+\n",
      "|date_sub(today, 5)|date_add(today, 5)|\n",
      "+------------------+------------------+\n",
      "|        2023-05-09|        2023-05-19|\n",
      "+------------------+------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import date_add, date_sub\n",
    "dateDF.select(date_sub(col(\"today\"), 5), date_add(col(\"today\"), 5)).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "fb603fec-0d8d-4b66-b69e-a9f7fb3f6be2",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Another common task is to take a look at the difference between two dates. We can do this with the\n",
    "datediff function that will return the number of days in between two dates. Most often we just care\n",
    "about the days, and because the number of days varies from month to month, there also exists a\n",
    "function, months_between, that gives you the number of months between two dates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "81357554-f9a9-45f4-b45e-e504d0bcd772",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+\n",
      "|datediff(week_ago, today)|\n",
      "+-------------------------+\n",
      "|                       -7|\n",
      "+-------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import datediff, months_between, to_date\n",
    "dateDF.withColumn(\"week_ago\", date_sub(col(\"today\"), 7))\\\n",
    "  .select(datediff(col(\"week_ago\"), col(\"today\"))).show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------------+\n",
      "|months_between(start, end, true)|\n",
      "+--------------------------------+\n",
      "|                     -4.29032258|\n",
      "+--------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dateDF.select(\n",
    "    to_date(lit(\"2023-01-01\")).alias(\"start\"),\n",
    "    to_date(lit(\"2023-05-10\")).alias(\"end\"))\\\n",
    "  .select(months_between(col(\"start\"), col(\"end\"))).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c2ca26d7-f63c-4031-9667-55c2bdefe0d9",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "Notice that we introduced a new function: the to_date function. The to_date function allows you to\n",
    "convert a string to a date, optionally with a specified format. We specify our format in the Java\n",
    "SimpleDateFormat which will be important to reference if you use this function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8b200b48-eb98-4d33-9b5a-22692bd2dcb7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|to_date(`date`)|\n",
      "+---------------+\n",
      "|     2023-01-01|\n",
      "+---------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "from pyspark.sql.functions import to_date, lit\n",
    "spark.range(5).withColumn(\"date\", lit(\"2023-01-01\"))\\\n",
    "  .select(to_date(col(\"date\"))).show(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "1aa09515-d1ee-4c11-861d-5124f2658e99",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "# Working with Nulls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "70593949-cb1f-4129-925e-317a6a91b79f",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## drop\n",
    "The simplest function is drop, which removes rows that contain nulls. The default is to drop any row\n",
    "in which any value is null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 28233 entries, 0 to 28232\n",
      "Data columns (total 25 columns):\n",
      " #   Column                 Non-Null Count  Dtype  \n",
      "---  ------                 --------------  -----  \n",
      " 0   ARR_AIRPORT_LAT        28233 non-null  float64\n",
      " 1   ARR_AIRPORT_LON        28233 non-null  float64\n",
      " 2   ARR_AIRPORT_TZOFFSET   28233 non-null  float64\n",
      " 3   ARR_DELAY              28012 non-null  float64\n",
      " 4   ARR_TIME               28233 non-null  object \n",
      " 5   CANCELLED              28233 non-null  bool   \n",
      " 6   CRS_ARR_TIME           28233 non-null  object \n",
      " 7   CRS_DEP_TIME           28233 non-null  object \n",
      " 8   DEP_AIRPORT_LAT        28233 non-null  float64\n",
      " 9   DEP_AIRPORT_LON        28233 non-null  float64\n",
      " 10  DEP_AIRPORT_TZOFFSET   28233 non-null  float64\n",
      " 11  DEP_DELAY              28068 non-null  float64\n",
      " 12  DEP_TIME               28233 non-null  object \n",
      " 13  DEST                   28233 non-null  object \n",
      " 14  DEST_AIRPORT_SEQ_ID    28233 non-null  object \n",
      " 15  DISTANCE               28233 non-null  object \n",
      " 16  DIVERTED               28233 non-null  bool   \n",
      " 17  FL_DATE                28233 non-null  object \n",
      " 18  ORIGIN                 28233 non-null  object \n",
      " 19  ORIGIN_AIRPORT_SEQ_ID  28233 non-null  object \n",
      " 20  TAXI_IN                28051 non-null  float64\n",
      " 21  TAXI_OUT               28057 non-null  float64\n",
      " 22  UNIQUE_CARRIER         28233 non-null  object \n",
      " 23  WHEELS_OFF             28233 non-null  object \n",
      " 24  WHEELS_ON              28233 non-null  object \n",
      "dtypes: bool(2), float64(10), object(13)\n",
      "memory usage: 5.0+ MB\n"
     ]
    }
   ],
   "source": [
    "df.toPandas().info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "c3aefabc-80cb-41b0-a8b5-c97769fa74d3",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ARR_AIRPORT_LAT: double, ARR_AIRPORT_LON: double, ARR_AIRPORT_TZOFFSET: double, ARR_DELAY: double, ARR_TIME: string, CANCELLED: boolean, CRS_ARR_TIME: string, CRS_DEP_TIME: string, DEP_AIRPORT_LAT: double, DEP_AIRPORT_LON: double, DEP_AIRPORT_TZOFFSET: double, DEP_DELAY: double, DEP_TIME: string, DEST: string, DEST_AIRPORT_SEQ_ID: string, DISTANCE: string, DIVERTED: boolean, FL_DATE: string, ORIGIN: string, ORIGIN_AIRPORT_SEQ_ID: string, TAXI_IN: double, TAXI_OUT: double, UNIQUE_CARRIER: string, WHEELS_OFF: string, WHEELS_ON: string]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.na.drop(\"all\", subset=[\"DEP_DELAY\", \"ARR_DELAY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "201729dd-f57e-4d6d-ad6c-5afe2d0fcf94",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## fill\n",
    "Using the fill function, you can fill one or more columns with a set of values. This can be done by\n",
    "specifying a map—that is a particular value and a set of columns.For example, to fill all null values in columns of type String, you might specify the following:\n",
    "`df.na.fill(\"All Null values become this string\")`\n",
    "We could do the same for columns of type Integer by using `df.na.fill(5:Integer)`, or for Doubles\n",
    "`df.na.fill(5:Double)`. To specify columns, we just pass in an array of column names like we did\n",
    "in the previous example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8f735417-295a-4761-8726-43d4aa5dbb45",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ARR_AIRPORT_LAT: double, ARR_AIRPORT_LON: double, ARR_AIRPORT_TZOFFSET: double, ARR_DELAY: double, ARR_TIME: string, CANCELLED: boolean, CRS_ARR_TIME: string, CRS_DEP_TIME: string, DEP_AIRPORT_LAT: double, DEP_AIRPORT_LON: double, DEP_AIRPORT_TZOFFSET: double, DEP_DELAY: double, DEP_TIME: string, DEST: string, DEST_AIRPORT_SEQ_ID: string, DISTANCE: string, DIVERTED: boolean, FL_DATE: string, ORIGIN: string, ORIGIN_AIRPORT_SEQ_ID: string, TAXI_IN: double, TAXI_OUT: double, UNIQUE_CARRIER: string, WHEELS_OFF: string, WHEELS_ON: string]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.na.fill(0, subset=[\"DEP_DELAY\", \"ARR_DELAY\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "334fc440-2ff8-4084-b1a2-7c046bac062b",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "We can also do this with with a Map, where the key is the column name and the value is the\n",
    "value we would like to use to fill null values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "21f2b498-724e-46e6-b7bf-e31e534bf4f6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ARR_AIRPORT_LAT: double, ARR_AIRPORT_LON: double, ARR_AIRPORT_TZOFFSET: double, ARR_DELAY: double, ARR_TIME: string, CANCELLED: boolean, CRS_ARR_TIME: string, CRS_DEP_TIME: string, DEP_AIRPORT_LAT: double, DEP_AIRPORT_LON: double, DEP_AIRPORT_TZOFFSET: double, DEP_DELAY: double, DEP_TIME: string, DEST: string, DEST_AIRPORT_SEQ_ID: string, DISTANCE: string, DIVERTED: boolean, FL_DATE: string, ORIGIN: string, ORIGIN_AIRPORT_SEQ_ID: string, TAXI_IN: double, TAXI_OUT: double, UNIQUE_CARRIER: string, WHEELS_OFF: string, WHEELS_ON: string]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "fill_cols_vals = {\"DEP_DELAY\": 5, \"ARR_DELAY\" : \"No Value\"}\n",
    "df.na.fill(fill_cols_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "ff4ba009-2902-4479-8907-92513d531b22",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "## replace\n",
    "In addition to replacing null values like we did with drop and fill, there are more flexible options\n",
    "that you can use with more than just null values. Probably the most common use case is to replace all\n",
    "values in a certain column according to their current value. The only requirement is that this value be\n",
    "the same type as the original value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "33f4a7df-4a5d-46d3-a110-8a7906649f34",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[ARR_AIRPORT_LAT: double, ARR_AIRPORT_LON: double, ARR_AIRPORT_TZOFFSET: double, ARR_DELAY: double, ARR_TIME: string, CANCELLED: boolean, CRS_ARR_TIME: string, CRS_DEP_TIME: string, DEP_AIRPORT_LAT: double, DEP_AIRPORT_LON: double, DEP_AIRPORT_TZOFFSET: double, DEP_DELAY: double, DEP_TIME: string, DEST: string, DEST_AIRPORT_SEQ_ID: string, DISTANCE: string, DIVERTED: boolean, FL_DATE: string, ORIGIN: string, ORIGIN_AIRPORT_SEQ_ID: string, TAXI_IN: double, TAXI_OUT: double, UNIQUE_CARRIER: string, WHEELS_OFF: string, WHEELS_ON: string]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# COMMAND ----------\n",
    "\n",
    "df.na.replace([\"PHL\"], [\"UNKNOWN\"], \"DEST\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "d063f99c-522b-4a4c-ba61-684b52cf0789",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "INFO323-Lecture-presented-week6-Spark-6-Working-Different-Types-Data",
   "notebookOrigID": 4360609463330518,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}