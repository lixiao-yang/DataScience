{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1 style=\"text-align:center\"> INFO 323: Cloud Computing and Big Data</h1>\n",
    "<h2 style=\"text-align:center\"> College of Computing and Informatics</h2>\n",
    "<h2 style=\"text-align:center\">Drexel University</h2>\n",
    "\n",
    "<h3 style=\"text-align:center\"> Introduction to Spark Programming</h3>\n",
    "<h3 style=\"text-align:center\"> Yuan An, PhD</h3>\n",
    "<h3 style=\"text-align:center\">Associate Professor</h3>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Computing Recap\n",
    "- Typically, when we think of a “computer,” we think about one machine sitting on our desk at home or at work.\n",
    "- There are some things that our computer is not powerful enough to perform. One particularly challenging area is data processing.\n",
    "- Single machines do not have enough power and resources to perform computations on huge amounts of information (or the user probably does not have the time to wait for the computation to finish). \n",
    "- A cluster, or group, of computers, pools the resources of many machines together, giving us the ability to use all the cumulative resources as if they were a single computer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Computing and Spark\n",
    "- Now, a group of machines alone is not powerful, we need a framework to coordinate work across them. \n",
    "- Spark does just that, managing and coordinating the execution of tasks on data across a cluster of computers.\n",
    "- The cluster of machines that Spark will use to execute tasks is managed by a cluster manager like\n",
    " - * Spark’s standalone cluster manager, \n",
    " - * YARN, \n",
    " - * or Mesos. \n",
    "- We then submit Spark Applications to these cluster managers, which will grant resources to our application so that we can complete our work.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Cluster\n",
    "Spark cluster consists of driver and worker nodes.\n",
    "![](https://i.imgur.com/6I4l6ZH.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Applications\n",
    "Spark Applications consist of a driver process and a set of executor processes. \n",
    "![](https://i.imgur.com/zF7Ngfc.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Executor\n",
    "Each executor’s core gets a partition of data to work on\n",
    "![](https://i.imgur.com/FZN5dCb.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Driver\n",
    "- Each Spark driver creates one or more Spark jobs; \n",
    "- Each Spark job creates one or more stages; \n",
    "- Each Spark stage creates one or more tasks to be distributed to executors\n",
    "![](https://i.imgur.com/lMTRd1c.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Programs\n",
    "A program consists of a sequence of transformations followed by an action.\n",
    "![](https://i.imgur.com/NLTWLo0.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark RDD\n",
    "- The primary data abstraction structure for Spark applications, is one of the main differentiators between Spark and other cluster computing frameworks. \n",
    "- In-memory collections of data distributed across a cluster. \n",
    "- Spark programs using the Spark core API consist of\n",
    "- * loading input data into an RDD\n",
    "- * transforming the RDD into subsequent RDDs\n",
    "- * storing or presenting the final output for an application from the resulting final RDD.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrames\n",
    "The most common Structured API and simply represents a table of data with rows and columns\n",
    "![](https://i.imgur.com/qvdB1rT.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformations\n",
    "- In Spark, the core data structures are immutable, meaning they cannot be changed after they’re created. \n",
    "- This might seem like a strange concept at first: if we cannot change it, how are we supposed to use it? \n",
    "- To “change” a DataFrame, we need to instruct Spark how we would like to modify it to do what we want.\n",
    "- These instructions are called transformations. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Narrow Transformation\n",
    "Transformations consisting of narrow dependencies (we’ll call them narrow transformations) are\n",
    "those for which each input partition will contribute to only one output partition.\n",
    "# Wide Transformation\n",
    "A wide dependency (or wide transformation) style transformation will have input partitions\n",
    "contributing to many output partitions. You will often hear this referred to as a shuffle whereby Spark\n",
    "will exchange partitions across the cluster. With narrow transformations, Spark will automatically\n",
    "perform an operation called pipelining, meaning that if we specify multiple filters on DataFrames,\n",
    "they’ll all be performed in-memory. The same cannot be said for shuffles. When we perform a shuffle,\n",
    "Spark writes the results to disk. \n",
    "![narrow vs wide](https://i.imgur.com/jJ4fypS.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lazy Evaluation\n",
    "Lazy evaluation means that Spark will wait until the very last moment to execute the graph of\n",
    "computation instructions. \n",
    "\n",
    "In Spark, instead of modifying the data immediately when you express some\n",
    "operation, you build up a plan of transformations that you would like to apply to your source data. \n",
    "\n",
    "By waiting until the last minute to execute the code, Spark compiles this plan from your raw DataFrame\n",
    "transformations to a streamlined physical plan that will run as efficiently as possible across the\n",
    "cluster. \n",
    "\n",
    "This provides immense benefits because Spark can optimize the entire data flow from end to\n",
    "end. \n",
    "\n",
    "An example of this is something called predicate pushdown on DataFrames. If we build a large\n",
    "Spark job but specify a filter at the end that only requires us to fetch one row from our source data,\n",
    "the most efficient way to execute this is to access the single record that we need. Spark will actually\n",
    "optimize this for us by pushing the filter down automatically.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actions\n",
    "- To trigger the computation, we run an action - instructs Spark to compute a result from a series of transformations. \n",
    "- There are three kinds of actions:\n",
    "- * Actions to view data in the console\n",
    "- * Actions to collect data to native objects in the respective language\n",
    "- * Actions to write to output data sources\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataFrame Example\n",
    "DataFrame consists of a series of records (like rows in a table), that are of type Row,\n",
    "and a number of columns (like columns in a spreadsheet) that represent a computation expression that\n",
    "can be performed on each individual record in the Dataset. \n",
    "\n",
    "Schemas define the name as well as the\n",
    "type of data in each column. \n",
    "\n",
    "Partitioning of the DataFrame defines the layout of the DataFrame or\n",
    "Dataset’s physical distribution across the cluster. \n",
    "\n",
    "The partitioning scheme defines how that is\n",
    "allocated. You can set this to be based on values in a certain column or nondeterministically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "range_df = spark.range(500).toDF(\"number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(number=0), Row(number=1)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "range_df.take(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
